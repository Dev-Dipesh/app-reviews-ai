{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Date Parsing Issues\n",
    "\n",
    "This notebook investigates why date values are showing up as NaT in the review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/dipesh/Local-Projects/indigo-reviews-ai to Python path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file directly with a sample for testing\n",
    "csv_path = os.path.join(project_root, 'data', 'reviews.csv')\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"Loading reviews from: {csv_path}\")\n",
    "    # Load only a sample of 1000 reviews to speed up testing\n",
    "    raw_df = pd.read_csv(csv_path, low_memory=False, nrows=1000)\n",
    "    print(f\"Successfully loaded {len(raw_df)} sample reviews from CSV file\")\n",
    "else:\n",
    "    print(f\"CSV file not found at {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns and their types\n",
    "print(\"DataFrame Info:\")\n",
    "raw_df.info()\n",
    "\n",
    "# Focus on date-related columns\n",
    "date_cols = [col for col in raw_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"\\nPotential date columns: {date_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the date column\n",
    "if 'date' in raw_df.columns:\n",
    "    print(\"Sample values from 'date' column:\")\n",
    "    print(raw_df['date'].sample(10).tolist())\n",
    "    \n",
    "    # Check unique values\n",
    "    unique_dates = raw_df['date'].nunique()\n",
    "    print(f\"\\nNumber of unique date values: {unique_dates}\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_dates = raw_df['date'].isna().sum()\n",
    "    print(f\"Number of null date values: {null_dates} ({null_dates/len(raw_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Try to determine the date format if it's a string\n",
    "    if pd.api.types.is_object_dtype(raw_df['date']):\n",
    "        non_null_dates = raw_df['date'].dropna()\n",
    "        if len(non_null_dates) > 0:\n",
    "            print(f\"\\nFirst few non-null date values:\")\n",
    "            print(non_null_dates.head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if timestamp column exists and might be useful\n",
    "if 'timestamp' in raw_df.columns:\n",
    "    print(\"Sample values from 'timestamp' column:\")\n",
    "    print(raw_df['timestamp'].sample(10).tolist())\n",
    "    \n",
    "    # Check unique values\n",
    "    unique_timestamps = raw_df['timestamp'].nunique()\n",
    "    print(f\"\\nNumber of unique timestamp values: {unique_timestamps}\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_timestamps = raw_df['timestamp'].isna().sum()\n",
    "    print(f\"Number of null timestamp values: {null_timestamps} ({null_timestamps/len(raw_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Try different timestamp formats\n",
    "    print(\"\\nTrying different timestamp interpretations:\")\n",
    "    \n",
    "    # First, get non-null values\n",
    "    non_null_timestamps = raw_df['timestamp'].dropna()\n",
    "    \n",
    "    if len(non_null_timestamps) > 0:\n",
    "        sample_timestamp = non_null_timestamps.iloc[0]\n",
    "        \n",
    "        print(f\"Sample timestamp: {sample_timestamp}\")\n",
    "        \n",
    "        # Try as Unix timestamp (seconds since epoch)\n",
    "        try:\n",
    "            if isinstance(sample_timestamp, (int, float)):\n",
    "                unix_date = pd.to_datetime(sample_timestamp, unit='s')\n",
    "                print(f\"As Unix timestamp (seconds): {unix_date}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Not a Unix timestamp (seconds): {e}\")\n",
    "            \n",
    "        # Try as Unix timestamp in milliseconds\n",
    "        try:\n",
    "            if isinstance(sample_timestamp, (int, float)):\n",
    "                unix_ms_date = pd.to_datetime(sample_timestamp, unit='ms')\n",
    "                print(f\"As Unix timestamp (milliseconds): {unix_ms_date}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Not a Unix timestamp (milliseconds): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's another potential date source\n",
    "if 'repliedAt' in raw_df.columns:\n",
    "    print(\"Sample values from 'repliedAt' column:\")\n",
    "    print(raw_df['repliedAt'].sample(10).tolist())\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_replied = raw_df['repliedAt'].isna().sum()\n",
    "    print(f\"Number of null repliedAt values: {null_replied} ({null_replied/len(raw_df)*100:.2f}%)\")\n",
    "    \n",
    "    # If there are non-null values, check format\n",
    "    non_null_replied = raw_df['repliedAt'].dropna()\n",
    "    if len(non_null_replied) > 0:\n",
    "        print(f\"\\nFirst few non-null repliedAt values:\")\n",
    "        print(non_null_replied.head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to parse dates from multiple sources\n",
    "processed_df = raw_df.copy()\n",
    "\n",
    "# Attempt 1: Parse the date column if it's a string\n",
    "if 'date' in processed_df.columns and pd.api.types.is_object_dtype(processed_df['date']):\n",
    "    try:\n",
    "        print(\"Attempting to parse 'date' column as datetime...\")\n",
    "        processed_df['parsed_date_1'] = pd.to_datetime(processed_df['date'], errors='coerce')\n",
    "        valid_count = processed_df['parsed_date_1'].notna().sum()\n",
    "        print(f\"Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date column: {e}\")\n",
    "\n",
    "# Attempt 2: Parse the timestamp column as seconds since epoch\n",
    "if 'timestamp' in processed_df.columns:\n",
    "    try:\n",
    "        print(\"\\nAttempting to parse 'timestamp' column as seconds since epoch...\")\n",
    "        processed_df['parsed_date_2'] = pd.to_datetime(processed_df['timestamp'], unit='s', errors='coerce')\n",
    "        valid_count = processed_df['parsed_date_2'].notna().sum()\n",
    "        print(f\"Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing timestamp column (seconds): {e}\")\n",
    "        \n",
    "# Attempt 3: Parse the timestamp column as milliseconds since epoch\n",
    "if 'timestamp' in processed_df.columns:\n",
    "    try:\n",
    "        print(\"\\nAttempting to parse 'timestamp' column as milliseconds since epoch...\")\n",
    "        processed_df['parsed_date_3'] = pd.to_datetime(processed_df['timestamp'], unit='ms', errors='coerce')\n",
    "        valid_count = processed_df['parsed_date_3'].notna().sum()\n",
    "        print(f\"Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing timestamp column (milliseconds): {e}\")\n",
    "        \n",
    "# Show the results\n",
    "date_cols = [col for col in processed_df.columns if col.startswith('parsed_date_')]\n",
    "if date_cols:\n",
    "    print(\"\\nSample of parsed dates:\")\n",
    "    processed_df[date_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try custom parsers for string date formats\n",
    "if 'date' in processed_df.columns and pd.api.types.is_object_dtype(processed_df['date']):\n",
    "    # Get sample of non-null dates\n",
    "    non_null_dates = processed_df['date'].dropna()\n",
    "    \n",
    "    if len(non_null_dates) > 0:\n",
    "        print(\"Testing custom date formats on sample values...\")\n",
    "        sample_dates = non_null_dates.sample(min(5, len(non_null_dates))).tolist()\n",
    "        \n",
    "        # Common date formats to try\n",
    "        formats = [\n",
    "            '%Y-%m-%d',            # 2023-01-31\n",
    "            '%Y/%m/%d',            # 2023/01/31\n",
    "            '%d-%m-%Y',            # 31-01-2023\n",
    "            '%d/%m/%Y',            # 31/01/2023\n",
    "            '%m-%d-%Y',            # 01-31-2023\n",
    "            '%m/%d/%Y',            # 01/31/2023\n",
    "            '%Y-%m-%d %H:%M:%S',   # 2023-01-31 14:30:45\n",
    "            '%d-%m-%Y %H:%M:%S',   # 31-01-2023 14:30:45\n",
    "            '%m-%d-%Y %H:%M:%S',   # 01-31-2023 14:30:45\n",
    "            '%b %d, %Y',           # Jan 31, 2023\n",
    "            '%B %d, %Y',           # January 31, 2023\n",
    "            '%d %b %Y',            # 31 Jan 2023\n",
    "            '%d %B %Y',            # 31 January 2023\n",
    "            '%Y%m%d'               # 20230131\n",
    "        ]\n",
    "        \n",
    "        print(f\"Sample dates: {sample_dates}\")\n",
    "        \n",
    "        for date_format in formats:\n",
    "            successful = 0\n",
    "            for sample_date in sample_dates:\n",
    "                if isinstance(sample_date, str):\n",
    "                    try:\n",
    "                        parsed = datetime.strptime(sample_date, date_format)\n",
    "                        successful += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            if successful > 0:\n",
    "                print(f\"Format '{date_format}' worked for {successful}/{len(sample_dates)} samples\")\n",
    "                \n",
    "                # Try applying this format to the whole dataset\n",
    "                format_name = date_format.replace('%', '').replace(':', '').replace(' ', '_')\n",
    "                col_name = f'parsed_date_{format_name}'\n",
    "                \n",
    "                try:\n",
    "                    processed_df[col_name] = pd.to_datetime(processed_df['date'], format=date_format, errors='coerce')\n",
    "                    valid_count = processed_df[col_name].notna().sum()\n",
    "                    print(f\"  Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%) using {date_format}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error applying format {date_format}: {e}\")\n",
    "        \n",
    "        print(\"\\nChecking if any custom parsers worked well:\")\n",
    "        custom_date_cols = [col for col in processed_df.columns if col.startswith('parsed_date_') and col not in date_cols]\n",
    "        \n",
    "        for col in custom_date_cols:\n",
    "            valid_count = processed_df[col].notna().sum()\n",
    "            if valid_count > 0:\n",
    "                print(f\"{col}: {valid_count} valid dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "                print(processed_df[col].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some review records with version info to see if there's any correlation\n",
    "if 'version' in raw_df.columns:\n",
    "    print(\"Examining records with version info:\")\n",
    "    version_groups = raw_df.groupby('version').size().reset_index(name='count')\n",
    "    version_groups = version_groups.sort_values('count', ascending=False)\n",
    "    print(version_groups.head(10))\n",
    "    \n",
    "    # Check a few specific versions to see if they have date info\n",
    "    for version in version_groups['version'].head(3):\n",
    "        if pd.notna(version):\n",
    "            print(f\"\\nSample records for version {version}:\")\n",
    "            version_sample = raw_df[raw_df['version'] == version].sample(min(3, len(raw_df[raw_df['version'] == version])))\n",
    "            print(version_sample[['review_id', 'date', 'timestamp', 'version']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"SUMMARY OF DATE DEBUGGING:\")\n",
    "print(\"==========================\")\n",
    "\n",
    "if 'date' in raw_df.columns:\n",
    "    null_dates = raw_df['date'].isna().sum()\n",
    "    print(f\"Date column: {null_dates}/{len(raw_df)} null values ({null_dates/len(raw_df)*100:.2f}%)\")\n",
    "    if pd.api.types.is_object_dtype(raw_df['date']):\n",
    "        print(\"  Data type: string/object\")\n",
    "    else:\n",
    "        print(f\"  Data type: {raw_df['date'].dtype}\")\n",
    "\n",
    "if 'timestamp' in raw_df.columns:\n",
    "    null_timestamps = raw_df['timestamp'].isna().sum()\n",
    "    print(f\"Timestamp column: {null_timestamps}/{len(raw_df)} null values ({null_timestamps/len(raw_df)*100:.2f}%)\")\n",
    "    print(f\"  Data type: {raw_df['timestamp'].dtype}\")\n",
    "\n",
    "# Best parsed date column\n",
    "all_date_cols = [col for col in processed_df.columns if col.startswith('parsed_date_')]\n",
    "if all_date_cols:\n",
    "    best_col = None\n",
    "    best_valid_count = 0\n",
    "    \n",
    "    for col in all_date_cols:\n",
    "        valid_count = processed_df[col].notna().sum()\n",
    "        if valid_count > best_valid_count:\n",
    "            best_valid_count = valid_count\n",
    "            best_col = col\n",
    "    \n",
    "    if best_col:\n",
    "        print(f\"\\nBest parsed date column: {best_col}\")\n",
    "        print(f\"  Valid dates: {best_valid_count}/{len(processed_df)} ({best_valid_count/len(processed_df)*100:.2f}%)\")\n",
    "        if best_valid_count > 0:\n",
    "            print(\"  Date range:\")\n",
    "            print(f\"    Min: {processed_df[best_col].min()}\")\n",
    "            print(f\"    Max: {processed_df[best_col].max()}\")\n",
    "            \n",
    "            # Sample of valid dates\n",
    "            valid_dates = processed_df[processed_df[best_col].notna()]\n",
    "            if len(valid_dates) > 0:\n",
    "                print(\"\\n  Sample of valid dates:\")\n",
    "                print(valid_dates[[best_col]].head(5))\n",
    "else:\n",
    "    print(\"\\nNo successful date parsing attempts\")\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\nRECOMMENDATION:\")\n",
    "if all_date_cols and best_valid_count > 0:\n",
    "    print(f\"Use the {best_col} column for date information. It successfully parsed {best_valid_count}/{len(processed_df)} dates.\")\n",
    "    print(\"Update the data loading code to use this parsing approach.\")\n",
    "else:\n",
    "    print(\"The date information in the dataset appears to be missing or in an unrecognized format.\")\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Continue using synthetic dates for visualization purposes\")\n",
    "    print(\"2. Investigate the data source to determine the correct date format\")\n",
    "    print(\"3. Consider using other metadata (like version numbers) as a proxy for timeframes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
