{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Date Parsing Issues\n",
    "\n",
    "This notebook investigates why date values are showing up as NaT in the review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/dipesh/Local-Projects/indigo-reviews-ai to Python path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# First, try to fetch a small sample of fresh data directly from the Play Store\nfrom src.runner import ReviewAnalysisRunner\n\n# Initialize the runner\nrunner = ReviewAnalysisRunner()\nrunner._initialize_modules()\n\n# Fetch a small number of recent reviews for debugging\nprint(\"\\n=== Fetching fresh reviews directly from Google Play Store ===\")\ntry:\n    app_id = os.environ.get(\"APP_ID\", \"in.goindigo.android\").strip()\n    max_reviews = 10  # Small sample just for debugging\n    \n    print(f\"Fetching {max_reviews} reviews for app ID: {app_id}\")\n    fresh_reviews_df = runner.fetch_reviews(\n        app_id=app_id,\n        start_date=\"1 month ago\",  # Recent reviews\n        end_date=\"now\",\n        max_reviews=max_reviews\n    )\n    \n    # Display the fresh reviews\n    if fresh_reviews_df is not None and not fresh_reviews_df.empty:\n        print(f\"Successfully fetched {len(fresh_reviews_df)} fresh reviews\")\n        print(\"\\nColumns in the fresh data:\")\n        print(list(fresh_reviews_df.columns))\n        \n        # Focus on date-related columns\n        date_cols = [col for col in fresh_reviews_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n        for col in date_cols:\n            print(f\"\\n{col} column data types and values:\")\n            print(f\"Data type: {fresh_reviews_df[col].dtype}\")\n            print(\"Sample values:\")\n            print(fresh_reviews_df[col].head())\n        \n        # Examine the raw date objects directly\n        if 'date' in fresh_reviews_df.columns:\n            print(\"\\nExamining raw date objects directly:\")\n            for i, date_obj in enumerate(fresh_reviews_df['date'].head(5)):\n                print(f\"Row {i}: Type={type(date_obj)}, Value={date_obj}, Repr={repr(date_obj)}\")\n        \n        # Display a complete row for examination\n        print(\"\\nComplete sample row for examination:\")\n        sample_row = fresh_reviews_df.iloc[0] if len(fresh_reviews_df) > 0 else None\n        if sample_row is not None:\n            for col, val in sample_row.items():\n                print(f\"{col}: {val} (Type: {type(val)})\")\n    else:\n        print(\"No fresh reviews fetched. Using stored data for analysis.\")\nexcept Exception as e:\n    print(f\"Error fetching fresh reviews: {e}\")\n    print(f\"Stack trace:\")\n    import traceback\n    traceback.print_exc()\n    print(\"\\nContinuing with existing data...\")\n\n# Load the CSV file with a sample for testing (as fallback)\ncsv_path = os.path.join(project_root, 'data', 'reviews.csv')\nif os.path.exists(csv_path):\n    print(f\"\\nLoading reviews from CSV as fallback: {csv_path}\")\n    # Load only a sample of 1000 reviews to speed up testing\n    raw_df = pd.read_csv(csv_path, low_memory=False, nrows=1000)\n    print(f\"Successfully loaded {len(raw_df)} sample reviews from CSV file\")\nelse:\n    print(f\"CSV file not found at {csv_path}\")\n    if 'fresh_reviews_df' in locals() and fresh_reviews_df is not None and not fresh_reviews_df.empty:\n        raw_df = fresh_reviews_df\n        print(\"Using fresh reviews as primary dataset.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns and their types\n",
    "print(\"DataFrame Info:\")\n",
    "raw_df.info()\n",
    "\n",
    "# Focus on date-related columns\n",
    "date_cols = [col for col in raw_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"\\nPotential date columns: {date_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the date column\n",
    "if 'date' in raw_df.columns:\n",
    "    print(\"Sample values from 'date' column:\")\n",
    "    print(raw_df['date'].sample(10).tolist())\n",
    "    \n",
    "    # Check unique values\n",
    "    unique_dates = raw_df['date'].nunique()\n",
    "    print(f\"\\nNumber of unique date values: {unique_dates}\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_dates = raw_df['date'].isna().sum()\n",
    "    print(f\"Number of null date values: {null_dates} ({null_dates/len(raw_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Try to determine the date format if it's a string\n",
    "    if pd.api.types.is_object_dtype(raw_df['date']):\n",
    "        non_null_dates = raw_df['date'].dropna()\n",
    "        if len(non_null_dates) > 0:\n",
    "            print(f\"\\nFirst few non-null date values:\")\n",
    "            print(non_null_dates.head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if timestamp column exists and might be useful\n",
    "if 'timestamp' in raw_df.columns:\n",
    "    print(\"Sample values from 'timestamp' column:\")\n",
    "    print(raw_df['timestamp'].sample(10).tolist())\n",
    "    \n",
    "    # Check unique values\n",
    "    unique_timestamps = raw_df['timestamp'].nunique()\n",
    "    print(f\"\\nNumber of unique timestamp values: {unique_timestamps}\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_timestamps = raw_df['timestamp'].isna().sum()\n",
    "    print(f\"Number of null timestamp values: {null_timestamps} ({null_timestamps/len(raw_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Try different timestamp formats\n",
    "    print(\"\\nTrying different timestamp interpretations:\")\n",
    "    \n",
    "    # First, get non-null values\n",
    "    non_null_timestamps = raw_df['timestamp'].dropna()\n",
    "    \n",
    "    if len(non_null_timestamps) > 0:\n",
    "        sample_timestamp = non_null_timestamps.iloc[0]\n",
    "        \n",
    "        print(f\"Sample timestamp: {sample_timestamp}\")\n",
    "        \n",
    "        # Try as Unix timestamp (seconds since epoch)\n",
    "        try:\n",
    "            if isinstance(sample_timestamp, (int, float)):\n",
    "                unix_date = pd.to_datetime(sample_timestamp, unit='s')\n",
    "                print(f\"As Unix timestamp (seconds): {unix_date}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Not a Unix timestamp (seconds): {e}\")\n",
    "            \n",
    "        # Try as Unix timestamp in milliseconds\n",
    "        try:\n",
    "            if isinstance(sample_timestamp, (int, float)):\n",
    "                unix_ms_date = pd.to_datetime(sample_timestamp, unit='ms')\n",
    "                print(f\"As Unix timestamp (milliseconds): {unix_ms_date}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Not a Unix timestamp (milliseconds): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's another potential date source\n",
    "if 'repliedAt' in raw_df.columns:\n",
    "    print(\"Sample values from 'repliedAt' column:\")\n",
    "    print(raw_df['repliedAt'].sample(10).tolist())\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_replied = raw_df['repliedAt'].isna().sum()\n",
    "    print(f\"Number of null repliedAt values: {null_replied} ({null_replied/len(raw_df)*100:.2f}%)\")\n",
    "    \n",
    "    # If there are non-null values, check format\n",
    "    non_null_replied = raw_df['repliedAt'].dropna()\n",
    "    if len(non_null_replied) > 0:\n",
    "        print(f\"\\nFirst few non-null repliedAt values:\")\n",
    "        print(non_null_replied.head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to parse dates from multiple sources\n",
    "processed_df = raw_df.copy()\n",
    "\n",
    "# Attempt 1: Parse the date column if it's a string\n",
    "if 'date' in processed_df.columns and pd.api.types.is_object_dtype(processed_df['date']):\n",
    "    try:\n",
    "        print(\"Attempting to parse 'date' column as datetime...\")\n",
    "        processed_df['parsed_date_1'] = pd.to_datetime(processed_df['date'], errors='coerce')\n",
    "        valid_count = processed_df['parsed_date_1'].notna().sum()\n",
    "        print(f\"Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date column: {e}\")\n",
    "\n",
    "# Attempt 2: Parse the timestamp column as seconds since epoch\n",
    "if 'timestamp' in processed_df.columns:\n",
    "    try:\n",
    "        print(\"\\nAttempting to parse 'timestamp' column as seconds since epoch...\")\n",
    "        processed_df['parsed_date_2'] = pd.to_datetime(processed_df['timestamp'], unit='s', errors='coerce')\n",
    "        valid_count = processed_df['parsed_date_2'].notna().sum()\n",
    "        print(f\"Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing timestamp column (seconds): {e}\")\n",
    "        \n",
    "# Attempt 3: Parse the timestamp column as milliseconds since epoch\n",
    "if 'timestamp' in processed_df.columns:\n",
    "    try:\n",
    "        print(\"\\nAttempting to parse 'timestamp' column as milliseconds since epoch...\")\n",
    "        processed_df['parsed_date_3'] = pd.to_datetime(processed_df['timestamp'], unit='ms', errors='coerce')\n",
    "        valid_count = processed_df['parsed_date_3'].notna().sum()\n",
    "        print(f\"Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing timestamp column (milliseconds): {e}\")\n",
    "        \n",
    "# Show the results\n",
    "date_cols = [col for col in processed_df.columns if col.startswith('parsed_date_')]\n",
    "if date_cols:\n",
    "    print(\"\\nSample of parsed dates:\")\n",
    "    processed_df[date_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try custom parsers for string date formats\n",
    "if 'date' in processed_df.columns and pd.api.types.is_object_dtype(processed_df['date']):\n",
    "    # Get sample of non-null dates\n",
    "    non_null_dates = processed_df['date'].dropna()\n",
    "    \n",
    "    if len(non_null_dates) > 0:\n",
    "        print(\"Testing custom date formats on sample values...\")\n",
    "        sample_dates = non_null_dates.sample(min(5, len(non_null_dates))).tolist()\n",
    "        \n",
    "        # Common date formats to try\n",
    "        formats = [\n",
    "            '%Y-%m-%d',            # 2023-01-31\n",
    "            '%Y/%m/%d',            # 2023/01/31\n",
    "            '%d-%m-%Y',            # 31-01-2023\n",
    "            '%d/%m/%Y',            # 31/01/2023\n",
    "            '%m-%d-%Y',            # 01-31-2023\n",
    "            '%m/%d/%Y',            # 01/31/2023\n",
    "            '%Y-%m-%d %H:%M:%S',   # 2023-01-31 14:30:45\n",
    "            '%d-%m-%Y %H:%M:%S',   # 31-01-2023 14:30:45\n",
    "            '%m-%d-%Y %H:%M:%S',   # 01-31-2023 14:30:45\n",
    "            '%b %d, %Y',           # Jan 31, 2023\n",
    "            '%B %d, %Y',           # January 31, 2023\n",
    "            '%d %b %Y',            # 31 Jan 2023\n",
    "            '%d %B %Y',            # 31 January 2023\n",
    "            '%Y%m%d'               # 20230131\n",
    "        ]\n",
    "        \n",
    "        print(f\"Sample dates: {sample_dates}\")\n",
    "        \n",
    "        for date_format in formats:\n",
    "            successful = 0\n",
    "            for sample_date in sample_dates:\n",
    "                if isinstance(sample_date, str):\n",
    "                    try:\n",
    "                        parsed = datetime.strptime(sample_date, date_format)\n",
    "                        successful += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            if successful > 0:\n",
    "                print(f\"Format '{date_format}' worked for {successful}/{len(sample_dates)} samples\")\n",
    "                \n",
    "                # Try applying this format to the whole dataset\n",
    "                format_name = date_format.replace('%', '').replace(':', '').replace(' ', '_')\n",
    "                col_name = f'parsed_date_{format_name}'\n",
    "                \n",
    "                try:\n",
    "                    processed_df[col_name] = pd.to_datetime(processed_df['date'], format=date_format, errors='coerce')\n",
    "                    valid_count = processed_df[col_name].notna().sum()\n",
    "                    print(f\"  Successfully parsed {valid_count} dates ({valid_count/len(processed_df)*100:.2f}%) using {date_format}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error applying format {date_format}: {e}\")\n",
    "        \n",
    "        print(\"\\nChecking if any custom parsers worked well:\")\n",
    "        custom_date_cols = [col for col in processed_df.columns if col.startswith('parsed_date_') and col not in date_cols]\n",
    "        \n",
    "        for col in custom_date_cols:\n",
    "            valid_count = processed_df[col].notna().sum()\n",
    "            if valid_count > 0:\n",
    "                print(f\"{col}: {valid_count} valid dates ({valid_count/len(processed_df)*100:.2f}%)\")\n",
    "                print(processed_df[col].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some review records with version info to see if there's any correlation\n",
    "if 'version' in raw_df.columns:\n",
    "    print(\"Examining records with version info:\")\n",
    "    version_groups = raw_df.groupby('version').size().reset_index(name='count')\n",
    "    version_groups = version_groups.sort_values('count', ascending=False)\n",
    "    print(version_groups.head(10))\n",
    "    \n",
    "    # Check a few specific versions to see if they have date info\n",
    "    for version in version_groups['version'].head(3):\n",
    "        if pd.notna(version):\n",
    "            print(f\"\\nSample records for version {version}:\")\n",
    "            version_sample = raw_df[raw_df['version'] == version].sample(min(3, len(raw_df[raw_df['version'] == version])))\n",
    "            print(version_sample[['review_id', 'date', 'timestamp', 'version']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Examine the raw API data to check date structures\nprint(\"\\n=== EXAMINING RAW API DATA ===\")\n\ndef inspect_acquisition_module():\n    \"\"\"Access the raw API data from the acquisition module to examine date handling\"\"\"\n    if hasattr(runner, 'acquisition') and runner.acquisition is not None:\n        print(\"Accessing acquisition module directly...\")\n        \n        try:\n            # Try to get app info and raw reviews from API\n            app_info = runner.acquisition.get_app_info()\n            print(f\"Retrieved app info: {app_info}\")\n            \n            # Make a direct call to the API\n            print(\"\\nMaking a direct call to fetch 5 raw reviews...\")\n            raw_reviews = runner.acquisition._fetch_reviews(\n                app_id=os.environ.get(\"APP_ID\", \"in.goindigo.android\").strip(),\n                lang=\"en\",\n                country=\"us\",\n                sort=\"newest\",\n                count=5\n            )\n            \n            if raw_reviews:\n                print(f\"Successfully fetched {len(raw_reviews)} raw reviews\")\n                \n                # Examine the raw structure\n                for i, review in enumerate(raw_reviews):\n                    print(f\"\\nReview {i+1}:\")\n                    # Print all fields of interest\n                    print(f\"  Review ID: {review.get('reviewId', 'N/A')}\")\n                    print(f\"  Author: {review.get('userName', 'N/A')}\")\n                    print(f\"  Rating: {review.get('score', 'N/A')}\")\n                    \n                    # Especially focus on date-related fields\n                    print(f\"  Raw date field: {review.get('at', 'N/A')} (Type: {type(review.get('at', None))})\")\n                    if 'at' in review:\n                        print(f\"  Raw date field representation: {repr(review['at'])}\")\n                        \n                    # Check for other possible date fields\n                    time_millis = review.get('reviewCreatedVersion', 'N/A')\n                    print(f\"  reviewCreatedVersion: {time_millis} (Type: {type(time_millis)})\")\n                    \n                    # Look for timestamp or unix time\n                    unix_time = review.get('timeMillis', 'N/A')\n                    print(f\"  timeMillis: {unix_time} (Type: {type(unix_time)})\")\n                    \n                    if isinstance(unix_time, (int, float)):\n                        # Try different interpretations\n                        try:\n                            seconds_date = pd.to_datetime(unix_time, unit='s')\n                            print(f\"  As seconds since epoch: {seconds_date}\")\n                        except Exception as e:\n                            print(f\"  Not a valid seconds timestamp: {e}\")\n                            \n                        try:\n                            millis_date = pd.to_datetime(unix_time, unit='ms')\n                            print(f\"  As milliseconds since epoch: {millis_date}\")\n                        except Exception as e:\n                            print(f\"  Not a valid milliseconds timestamp: {e}\")\n                    \n                # Print a complete example to see the full structure\n                print(\"\\nComplete structure of one review (first):\")\n                import json\n                if raw_reviews:\n                    print(json.dumps(raw_reviews[0], indent=2, default=str))\n            else:\n                print(\"No raw reviews were returned\")\n                \n            # Check how the acquisition module transforms the reviews\n            print(\"\\nExamining the acquisition module's transformation process...\")\n            if hasattr(runner.acquisition, '_transform_review'):\n                # Get a raw review\n                raw_review = raw_reviews[0] if raw_reviews else None\n                \n                if raw_review:\n                    print(\"Transforming a raw review...\")\n                    try:\n                        # Call the transform method directly\n                        transformed = runner.acquisition._transform_review(raw_review)\n                        print(\"\\nTransformed review:\")\n                        for key, value in transformed.items():\n                            print(f\"  {key}: {value} (Type: {type(value)})\")\n                            \n                        # Specifically examine date transformation\n                        if 'date' in transformed:\n                            print(\"\\nFocus on date transformation:\")\n                            print(f\"  Original 'at': {raw_review.get('at', 'N/A')} (Type: {type(raw_review.get('at', None))})\")\n                            print(f\"  Original 'timeMillis': {raw_review.get('timeMillis', 'N/A')} (Type: {type(raw_review.get('timeMillis', None))})\")\n                            print(f\"  Transformed 'date': {transformed.get('date', 'N/A')} (Type: {type(transformed.get('date', None))})\")\n                    except Exception as e:\n                        print(f\"Error during transformation: {e}\")\n                        import traceback\n                        traceback.print_exc()\n                else:\n                    print(\"No raw review available for transformation test\")\n            else:\n                print(\"Transformation method not found in acquisition module\")\n        except Exception as e:\n            print(f\"Error inspecting acquisition module: {e}\")\n            import traceback\n            traceback.print_exc()\n    else:\n        print(\"Acquisition module not initialized or available\")\n\n# Run the acquisition inspection        \ninspect_acquisition_module()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check date handling in our source code\nprint(\"\\n=== EXAMINING SOURCE CODE DATE HANDLING ===\")\ntry:\n    # Try to find the specific code in acquisition module that handles dates\n    source_file = os.path.join(project_root, 'src', 'modules', 'acquisition', 'google_play.py')\n    \n    if os.path.exists(source_file):\n        print(f\"Looking for date handling in {source_file}\")\n        with open(source_file, 'r') as f:\n            source_code = f.read()\n            \n        # Find lines that might be related to date parsing\n        import re\n        date_patterns = [\n            r'.*date.*=.*',\n            r'.*time.*=.*',\n            r'.*at.*=.*',\n            r'.*parse.*',\n            r'.*datetime.*',\n            r'.*pd\\.to_datetime.*'\n        ]\n        \n        print(\"Lines potentially related to date handling:\")\n        found_lines = []\n        for pattern in date_patterns:\n            matches = re.findall(pattern, source_code, re.IGNORECASE)\n            for match in matches:\n                match = match.strip()\n                if match and match not in found_lines and not match.startswith('#'):\n                    found_lines.append(match)\n                    \n        # Sort lines to group related code\n        found_lines.sort()\n        for line in found_lines:\n            print(f\"  {line}\")\n    else:\n        print(f\"Source file not found: {source_file}\")\nexcept Exception as e:\n    print(f\"Error examining source code: {e}\")\n    \n# Suggest a solution\nprint(\"\\n=== SUGGESTED SOLUTIONS ===\")\nprint(\"Based on the investigation, here are potential solutions for the date parsing issue:\")\nprint(\"1. Check if the 'at' field from the API is properly converted to a datetime object\")\nprint(\"2. Try using 'timeMillis' from the raw API response as a milliseconds timestamp\")\nprint(\"3. Add explicit error handling in the date parsing code to better diagnose issues\")\nprint(\"4. Examine preprocessing hooks to ensure dates aren't being dropped during cleaning\")\nprint(\"5. Add a backup date parsing strategy if the primary method fails\")\n\n# Prototype a potential fix\nprint(\"\\n=== PROTOTYPE FIX ===\")\nprint(\"Here's a potential fix for the date parsing issue:\")\nprint(\"```python\")\nprint(\"# In the _transform_review method in google_play.py\")\nprint(\"def _transform_review(self, review):\")\nprint(\"    # ... existing code ... \")\nprint(\"    # Fix date parsing issues\")\nprint(\"    at_date = None\")\nprint(\"    try:\")\nprint(\"        # First try to parse the 'at' field if it exists\")\nprint(\"        if 'at' in review and review['at']:\")\nprint(\"            at_date = pd.to_datetime(review['at'])\")\nprint(\"    except Exception as e:\")\nprint(\"        print(f\\\"Warning: Could not parse 'at' field: {e}\\\")\")\nprint(\"    \")\nprint(\"    # If 'at' parsing failed, try timeMillis as a backup\")\nprint(\"    if at_date is None and 'timeMillis' in review:\")\nprint(\"        try:\")\nprint(\"            # Try parsing as milliseconds since epoch\")\nprint(\"            at_date = pd.to_datetime(review['timeMillis'], unit='ms')\")\nprint(\"        except Exception as e:\")\nprint(\"            print(f\\\"Warning: Could not parse 'timeMillis' field: {e}\\\")\")\nprint(\"    \")\nprint(\"    transformed_review['date'] = at_date\")\nprint(\"    # ... rest of the method ... \")\nprint(\"```\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test the proposed fix directly in the notebook\nprint(\"\\n=== TESTING PROPOSED FIX ===\")\n\ndef test_fix_with_mock_data():\n    \"\"\"Test our fix with mock API response data\"\"\"\n    # Create mock review data similar to what we'd get from the API\n    mock_reviews = [\n        {\n            'reviewId': 'mock_review_1',\n            'userName': 'Test User 1',\n            'score': 4,\n            'at': '2023-05-01T14:30:45Z',  # Standard ISO format\n            'timeMillis': 1682951445000,   # Same date in milliseconds\n        },\n        {\n            'reviewId': 'mock_review_2',\n            'userName': 'Test User 2',\n            'score': 3,\n            'at': 'May 15, 2023',          # Different format that might fail\n            'timeMillis': 1684159845000,   # May 15, 2023\n        },\n        {\n            'reviewId': 'mock_review_3',\n            'userName': 'Test User 3',\n            'score': 5,\n            'at': None,                    # Missing 'at' field\n            'timeMillis': 1686751845000,   # June 14, 2023\n        },\n        {\n            'reviewId': 'mock_review_4',\n            'userName': 'Test User 4',\n            'score': 2,\n            'at': 'Invalid date string',   # Invalid format\n            'timeMillis': 1689343845000,   # July 14, 2023\n        },\n        {\n            'reviewId': 'mock_review_5',\n            'userName': 'Test User 5',\n            'score': 1,\n            'at': '2023-08-14T10:15:30Z',  # Standard ISO format\n            'timeMillis': None,            # Missing timeMillis\n        }\n    ]\n    \n    print(f\"Testing with {len(mock_reviews)} mock reviews\")\n    \n    # Original processing approach (simulated)\n    original_results = []\n    for review in mock_reviews:\n        try:\n            # Simulate the current approach that might be failing\n            if 'at' in review and review['at']:\n                date = pd.to_datetime(review['at'])\n            else:\n                date = None\n                \n            original_results.append({\n                'review_id': review['reviewId'],\n                'date': date,\n                'rating': review['score']\n            })\n        except Exception as e:\n            original_results.append({\n                'review_id': review['reviewId'],\n                'date': None,  # Date parsing failed\n                'rating': review['score']\n            })\n            print(f\"Original approach failed for {review['reviewId']}: {e}\")\n    \n    # New approach with the fix\n    fixed_results = []\n    for review in mock_reviews:\n        at_date = None\n        try:\n            # First try to parse the 'at' field if it exists\n            if 'at' in review and review['at']:\n                at_date = pd.to_datetime(review['at'])\n        except Exception as e:\n            print(f\"Warning: Could not parse 'at' field for {review['reviewId']}: {e}\")\n        \n        # If 'at' parsing failed, try timeMillis as a backup\n        if at_date is None and 'timeMillis' in review and review['timeMillis']:\n            try:\n                # Try parsing as milliseconds since epoch\n                at_date = pd.to_datetime(review['timeMillis'], unit='ms')\n            except Exception as e:\n                print(f\"Warning: Could not parse 'timeMillis' field for {review['reviewId']}: {e}\")\n        \n        fixed_results.append({\n            'review_id': review['reviewId'],\n            'date': at_date,\n            'rating': review['score']\n        })\n    \n    # Compare results\n    print(\"\\nResults comparison:\")\n    print(\"| Review ID    | Original Date          | Fixed Date             |\")\n    print(\"|--------------|------------------------|------------------------|\")\n    for orig, fixed in zip(original_results, fixed_results):\n        orig_date = str(orig['date']) if orig['date'] is not None else \"None\"\n        fixed_date = str(fixed['date']) if fixed['date'] is not None else \"None\"\n        print(f\"| {orig['review_id']:<12} | {orig_date:<22} | {fixed_date:<22} |\")\n    \n    # Count successful parses\n    orig_success = sum(1 for r in original_results if r['date'] is not None)\n    fixed_success = sum(1 for r in fixed_results if r['date'] is not None)\n    \n    print(f\"\\nOriginal approach: {orig_success}/{len(mock_reviews)} successful date parses ({orig_success/len(mock_reviews)*100:.1f}%)\")\n    print(f\"Fixed approach: {fixed_success}/{len(mock_reviews)} successful date parses ({fixed_success/len(mock_reviews)*100:.1f}%)\")\n    \n    # Create DataFrames for comparison\n    orig_df = pd.DataFrame(original_results)\n    fixed_df = pd.DataFrame(fixed_results)\n    \n    print(\"\\nOriginal DataFrame:\")\n    print(orig_df)\n    \n    print(\"\\nFixed DataFrame:\")\n    print(fixed_df)\n    \n    return orig_df, fixed_df\n\n# Run the test with mock data\norig_df, fixed_df = test_fix_with_mock_data()\n\n# Try with real data if it was fetched\nif 'fresh_reviews_df' in locals() and fresh_reviews_df is not None and not fresh_reviews_df.empty:\n    print(\"\\n=== TESTING WITH REAL DATA ===\")\n    \n    # Get raw reviews directly from acquisition module if possible\n    try:\n        if hasattr(runner, 'acquisition') and runner.acquisition is not None:\n            # Make a direct call to the API again\n            raw_reviews = runner.acquisition._fetch_reviews(\n                app_id=os.environ.get(\"APP_ID\", \"in.goindigo.android\").strip(),\n                lang=\"en\",\n                country=\"us\",\n                sort=\"newest\",\n                count=5\n            )\n            \n            if raw_reviews:\n                print(f\"Testing the fix with {len(raw_reviews)} real reviews\")\n                \n                # Create DataFrame with the original approach\n                original_dates = []\n                for review in raw_reviews:\n                    try:\n                        if 'at' in review and review['at']:\n                            date = pd.to_datetime(review['at'])\n                        else:\n                            date = None\n                    except Exception:\n                        date = None\n                    \n                    original_dates.append({\n                        'review_id': review.get('reviewId', 'unknown'),\n                        'date': date\n                    })\n                \n                # Apply the fix\n                fixed_dates = []\n                for review in raw_reviews:\n                    at_date = None\n                    try:\n                        if 'at' in review and review['at']:\n                            at_date = pd.to_datetime(review['at'])\n                    except Exception:\n                        at_date = None\n                    \n                    if at_date is None and 'timeMillis' in review and review['timeMillis']:\n                        try:\n                            at_date = pd.to_datetime(review['timeMillis'], unit='ms')\n                        except Exception:\n                            at_date = None\n                    \n                    fixed_dates.append({\n                        'review_id': review.get('reviewId', 'unknown'),\n                        'date': at_date\n                    })\n                \n                # Compare results\n                print(\"\\nResults with real data:\")\n                print(\"| Review ID                | Original Date          | Fixed Date             |\")\n                print(\"|--------------------------|------------------------|------------------------|\")\n                for orig, fixed in zip(original_dates, fixed_dates):\n                    orig_date = str(orig['date']) if orig['date'] is not None else \"None\"\n                    fixed_date = str(fixed['date']) if fixed['date'] is not None else \"None\"\n                    print(f\"| {orig['review_id']:<24} | {orig_date:<22} | {fixed_date:<22} |\")\n                \n                # Count successful parses\n                orig_success = sum(1 for r in original_dates if r['date'] is not None)\n                fixed_success = sum(1 for r in fixed_dates if r['date'] is not None)\n                \n                print(f\"\\nOriginal approach: {orig_success}/{len(raw_reviews)} successful date parses ({orig_success/len(raw_reviews)*100:.1f}%)\")\n                print(f\"Fixed approach: {fixed_success}/{len(raw_reviews)} successful date parses ({fixed_success/len(raw_reviews)*100:.1f}%)\")\n            else:\n                print(\"No raw reviews available for testing with real data\")\n    except Exception as e:\n        print(f\"Error testing with real data: {e}\")\nelse:\n    print(\"\\nNo fresh reviews data available for testing the fix with real data\")\n\n# Final recommendations\nprint(\"\\n=== FINAL RECOMMENDATIONS ===\")\nprint(\"Based on the testing, the recommended approach is:\")\nprint(\"1. Modify the _transform_review method in google_play.py to implement the date parsing fix\")\nprint(\"2. Add explicit logging for date parsing failures to help diagnose issues\")\nprint(\"3. Add a unit test specifically for date parsing to ensure it works consistently\")\nprint(\"4. Consider using the timeMillis field as the primary date source since it's more reliable\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"SUMMARY OF DATE DEBUGGING:\")\n",
    "print(\"==========================\")\n",
    "\n",
    "if 'date' in raw_df.columns:\n",
    "    null_dates = raw_df['date'].isna().sum()\n",
    "    print(f\"Date column: {null_dates}/{len(raw_df)} null values ({null_dates/len(raw_df)*100:.2f}%)\")\n",
    "    if pd.api.types.is_object_dtype(raw_df['date']):\n",
    "        print(\"  Data type: string/object\")\n",
    "    else:\n",
    "        print(f\"  Data type: {raw_df['date'].dtype}\")\n",
    "\n",
    "if 'timestamp' in raw_df.columns:\n",
    "    null_timestamps = raw_df['timestamp'].isna().sum()\n",
    "    print(f\"Timestamp column: {null_timestamps}/{len(raw_df)} null values ({null_timestamps/len(raw_df)*100:.2f}%)\")\n",
    "    print(f\"  Data type: {raw_df['timestamp'].dtype}\")\n",
    "\n",
    "# Best parsed date column\n",
    "all_date_cols = [col for col in processed_df.columns if col.startswith('parsed_date_')]\n",
    "if all_date_cols:\n",
    "    best_col = None\n",
    "    best_valid_count = 0\n",
    "    \n",
    "    for col in all_date_cols:\n",
    "        valid_count = processed_df[col].notna().sum()\n",
    "        if valid_count > best_valid_count:\n",
    "            best_valid_count = valid_count\n",
    "            best_col = col\n",
    "    \n",
    "    if best_col:\n",
    "        print(f\"\\nBest parsed date column: {best_col}\")\n",
    "        print(f\"  Valid dates: {best_valid_count}/{len(processed_df)} ({best_valid_count/len(processed_df)*100:.2f}%)\")\n",
    "        if best_valid_count > 0:\n",
    "            print(\"  Date range:\")\n",
    "            print(f\"    Min: {processed_df[best_col].min()}\")\n",
    "            print(f\"    Max: {processed_df[best_col].max()}\")\n",
    "            \n",
    "            # Sample of valid dates\n",
    "            valid_dates = processed_df[processed_df[best_col].notna()]\n",
    "            if len(valid_dates) > 0:\n",
    "                print(\"\\n  Sample of valid dates:\")\n",
    "                print(valid_dates[[best_col]].head(5))\n",
    "else:\n",
    "    print(\"\\nNo successful date parsing attempts\")\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\nRECOMMENDATION:\")\n",
    "if all_date_cols and best_valid_count > 0:\n",
    "    print(f\"Use the {best_col} column for date information. It successfully parsed {best_valid_count}/{len(processed_df)} dates.\")\n",
    "    print(\"Update the data loading code to use this parsing approach.\")\n",
    "else:\n",
    "    print(\"The date information in the dataset appears to be missing or in an unrecognized format.\")\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Continue using synthetic dates for visualization purposes\")\n",
    "    print(\"2. Investigate the data source to determine the correct date format\")\n",
    "    print(\"3. Consider using other metadata (like version numbers) as a proxy for timeframes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}