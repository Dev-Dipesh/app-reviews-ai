{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook\n",
    "\n",
    "This notebook handles the complete lifecycle of data acquisition, cleaning, and preprocessing. It can:\n",
    "\n",
    "1. Fetch data from Google Play Store API (real data)\n",
    "2. Load data from existing CSV files (cached data)\n",
    "3. Generate mock data for testing purposes\n",
    "\n",
    "The output of this notebook is a clean, processed dataset stored in `processed_reviews.csv` that can be used by other notebooks for analysis and visualization.\n",
    "\n",
    "**Note:** You can control notebook behavior with these environment variables:\n",
    "- `USE_MOCK_DATA=true` - Use mock data instead of real API data\n",
    "- `FORCE_REFRESH=true` - Force fetching fresh data even if existing data is available\n",
    "- `MAX_REVIEWS=100` - Set the maximum number of reviews to fetch/process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/dipesh/Local-Projects/indigo-reviews-ai to Python path\n",
      "DEBUG: Applying APP_ID from environment: 'com.fss.indus'\n",
      "Configuration:\n",
      "\n",
      "- App ID: com.fss.indus\n",
      "- Max Reviews: 5000\n",
      "- Use Mock Data: False\n",
      "- Force Refresh: True\n",
      "- Raw Data Path: /Users/dipesh/Local-Projects/indigo-reviews-ai/data/reviews.csv\n",
      "- Processed Data Path: /Users/dipesh/Local-Projects/indigo-reviews-ai/data/processed/processed_reviews.csv\n",
      "\n",
      "Environment variables:\n",
      "- MAX_REVIEWS: 5000\n",
      "- USE_MOCK_DATA: false\n",
      "- FORCE_REFRESH: true\n",
      "- APP_ID: com.fss.indus\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import importlib\n",
    "import subprocess  # Add subprocess import\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Import project modules\n",
    "from src.runner import ReviewAnalysisRunner\n",
    "from src.modules.acquisition import google_play\n",
    "from src.modules.storage import file_storage\n",
    "from src.modules.preprocessing import nlp_preprocessor\n",
    "\n",
    "# Force reload modules to get latest changes\n",
    "importlib.reload(google_play)\n",
    "importlib.reload(file_storage)\n",
    "importlib.reload(nlp_preprocessor)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration and paths\n",
    "DATA_DIR = os.path.join(project_root, 'data')  # Updated to use /data instead of /src/data\n",
    "RAW_DATA_PATH = os.path.join(DATA_DIR, 'reviews.csv')\n",
    "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, 'processed', 'processed_reviews.csv')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(PROCESSED_DATA_PATH), exist_ok=True)\n",
    "\n",
    "# Environment variables\n",
    "USE_MOCK_DATA = os.environ.get('USE_MOCK_DATA', 'false').lower() in ('true', '1', 'yes', 'y')\n",
    "FORCE_REFRESH = os.environ.get('FORCE_REFRESH', 'false').lower() in ('true', '1', 'yes', 'y')\n",
    "MAX_REVIEWS = int(os.environ.get('MAX_REVIEWS', '100'))\n",
    "\n",
    "# Extract APP_ID properly (removing any comments)\n",
    "app_id_env = os.environ.get(\"APP_ID\", \"in.goindigo.android\")\n",
    "app_id = app_id_env.split('#')[0].strip()  # Remove any comments and whitespace\n",
    "\n",
    "print(f\"Configuration:\\n\")\n",
    "print(f\"- App ID: {app_id}\")\n",
    "print(f\"- Max Reviews: {MAX_REVIEWS}\")\n",
    "print(f\"- Use Mock Data: {USE_MOCK_DATA}\")\n",
    "print(f\"- Force Refresh: {FORCE_REFRESH}\")\n",
    "print(f\"- Raw Data Path: {RAW_DATA_PATH}\")\n",
    "print(f\"- Processed Data Path: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# Debug: Show environment variables as seen by Python\n",
    "print(\"\\nEnvironment variables:\")\n",
    "for env_var in ['MAX_REVIEWS', 'USE_MOCK_DATA', 'FORCE_REFRESH', 'APP_ID']:\n",
    "    print(f\"- {env_var}: {os.environ.get(env_var, 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "★ Data Acquisition Phase ★\n",
      "Force refresh requested, fetching fresh data...\n",
      "Fetching data directly using Google Play scraper...\n",
      "App info reports 408 total reviews available\n",
      "Note: This may be lower than actual available reviews\n",
      "Will attempt to fetch up to 5000 reviews\n",
      "\n",
      "Trying with language: en, country: us\n",
      "Fetching batch of 100 reviews... (total so far: 0)\n",
      "Retrieved 100 new reviews, total: 100/5000\n",
      "Fetching batch of 100 reviews... (total so far: 100)\n",
      "Retrieved 100 new reviews, total: 200/5000\n",
      "Fetching batch of 100 reviews... (total so far: 200)\n",
      "Retrieved 100 new reviews, total: 300/5000\n",
      "Fetching batch of 100 reviews... (total so far: 300)\n",
      "Retrieved 100 new reviews, total: 400/5000\n",
      "Fetching batch of 100 reviews... (total so far: 400)\n",
      "Retrieved 100 new reviews, total: 500/5000\n",
      "Fetching batch of 100 reviews... (total so far: 500)\n",
      "Retrieved 100 new reviews, total: 600/5000\n",
      "Fetching batch of 100 reviews... (total so far: 600)\n",
      "Retrieved 100 new reviews, total: 700/5000\n",
      "Fetching batch of 100 reviews... (total so far: 700)\n",
      "Retrieved 100 new reviews, total: 800/5000\n",
      "Fetching batch of 100 reviews... (total so far: 800)\n",
      "Retrieved 100 new reviews, total: 900/5000\n",
      "Fetching batch of 100 reviews... (total so far: 900)\n",
      "Retrieved 100 new reviews, total: 1000/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1000)\n",
      "Retrieved 100 new reviews, total: 1100/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1100)\n",
      "Retrieved 100 new reviews, total: 1200/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1200)\n",
      "Retrieved 100 new reviews, total: 1300/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1300)\n",
      "Retrieved 100 new reviews, total: 1400/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1400)\n",
      "Retrieved 100 new reviews, total: 1500/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1500)\n",
      "Retrieved 100 new reviews, total: 1600/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1600)\n",
      "Retrieved 100 new reviews, total: 1700/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1700)\n",
      "Retrieved 100 new reviews, total: 1800/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1800)\n",
      "Retrieved 100 new reviews, total: 1900/5000\n",
      "Fetching batch of 100 reviews... (total so far: 1900)\n",
      "Retrieved 100 new reviews, total: 2000/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2000)\n",
      "Retrieved 100 new reviews, total: 2100/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2100)\n",
      "Retrieved 100 new reviews, total: 2200/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2200)\n",
      "Retrieved 100 new reviews, total: 2300/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2300)\n",
      "Retrieved 100 new reviews, total: 2400/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2400)\n",
      "Retrieved 100 new reviews, total: 2500/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2500)\n",
      "Retrieved 100 new reviews, total: 2600/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2600)\n",
      "Retrieved 100 new reviews, total: 2700/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2700)\n",
      "Retrieved 100 new reviews, total: 2800/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2800)\n",
      "Retrieved 100 new reviews, total: 2900/5000\n",
      "Fetching batch of 100 reviews... (total so far: 2900)\n",
      "Retrieved 100 new reviews, total: 3000/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3000)\n",
      "Retrieved 100 new reviews, total: 3100/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3100)\n",
      "Retrieved 100 new reviews, total: 3200/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3200)\n",
      "Retrieved 100 new reviews, total: 3300/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3300)\n",
      "Retrieved 100 new reviews, total: 3400/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3400)\n",
      "Retrieved 100 new reviews, total: 3500/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3500)\n",
      "Retrieved 100 new reviews, total: 3600/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3600)\n",
      "Retrieved 100 new reviews, total: 3700/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3700)\n",
      "Retrieved 100 new reviews, total: 3800/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3800)\n",
      "Retrieved 100 new reviews, total: 3900/5000\n",
      "Fetching batch of 100 reviews... (total so far: 3900)\n",
      "Retrieved 100 new reviews, total: 4000/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4000)\n",
      "Retrieved 100 new reviews, total: 4100/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4100)\n",
      "Retrieved 100 new reviews, total: 4200/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4200)\n",
      "Retrieved 100 new reviews, total: 4300/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4300)\n",
      "Retrieved 100 new reviews, total: 4400/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4400)\n",
      "Retrieved 100 new reviews, total: 4500/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4500)\n",
      "Retrieved 100 new reviews, total: 4600/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4600)\n",
      "Retrieved 100 new reviews, total: 4700/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4700)\n",
      "Retrieved 100 new reviews, total: 4800/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4800)\n",
      "Retrieved 100 new reviews, total: 4900/5000\n",
      "Fetching batch of 100 reviews... (total so far: 4900)\n",
      "Retrieved 100 new reviews, total: 5000/5000\n",
      "Completed fetching 5000 reviews for en/us\n",
      "Reached target of 5000 reviews, stopping\n",
      "Completed fetching a total of 5000 reviews across all language/country combinations\n",
      "\n",
      "Successfully processed 5000 reviews directly from Google Play\n",
      "\n",
      "First 5 rows of fetched data:\n",
      "                              review_id                  author  rating                                                                                                                                                           text   version  thumbsUpCount replyContent repliedAt                date           timestamp\n",
      "0  15c7b93a-00a9-46ed-b5d0-ff21acee2c67            Shiv Shankar       5                                                                                                                                                           good  10.11.22              0         None       NaT 2025-05-07 18:18:21 2025-05-07 18:18:21\n",
      "1  2a6d3874-5be3-492b-954c-24390dfd2afe             Sunipa Nath       1  such an irritating app is this. you will open to do some important work and everytime it will redirect you to update some other app which is already updated.  10.11.22              0         None       NaT 2025-05-07 17:24:26 2025-05-07 17:24:26\n",
      "2  67f42ed4-4fb9-44f3-b548-023ee98eb488  SAINATH SOPANRAO PATIL       5                                                                                                                                                      good nice  10.11.22              0         None       NaT 2025-05-07 16:21:42 2025-05-07 16:21:42\n",
      "3  dc8bd34d-d4ee-40fd-8f91-932f472d6d43  Mahesh Reddy Gollamudi       1                                                                                                                             app is not opening, please rectify  10.11.22              0         None       NaT 2025-05-07 16:18:56 2025-05-07 16:18:56\n",
      "4  0ce29084-1e76-4846-8688-1819614b289f              Manu Kumar       1                                                                                    after upgrade its converted to worst app. and not supported for all mobiles      None              0         None       NaT 2025-05-07 16:14:14 2025-05-07 16:14:14\n",
      "Saved raw data to /Users/dipesh/Local-Projects/indigo-reviews-ai/data/reviews.csv\n",
      "\n",
      "Dataset Overview:\n",
      "- Shape: (5000, 10) (rows, columns)\n",
      "- Columns: ['review_id', 'author', 'rating', 'text', 'version', 'thumbsUpCount', 'replyContent', 'repliedAt', 'date', 'timestamp']\n",
      "- Date range: 2025-01-19 10:03:35 to 2025-05-07 18:18:21\n",
      "- Rating distribution:\n",
      "rating\n",
      "1    2624\n",
      "2     173\n",
      "3     166\n",
      "4     285\n",
      "5    1752\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Null values in key columns:\n",
      "review_id    0\n",
      "date         0\n",
      "rating       0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_existing_data():\n",
    "    \"\"\"Attempt to load existing data from reviews.csv\"\"\"\n",
    "    if os.path.exists(RAW_DATA_PATH):\n",
    "        try:\n",
    "            df = pd.read_csv(RAW_DATA_PATH)\n",
    "            # Convert date column to datetime\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            print(f\"Successfully loaded {len(df)} reviews from {RAW_DATA_PATH}\")\n",
    "            \n",
    "            # Display the first few rows of the loaded data\n",
    "            print(\"\\nFirst 5 rows of loaded data:\")\n",
    "            print(df.head().to_string())\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing data: {e}\")\n",
    "    print(f\"No existing data found at {RAW_DATA_PATH}\")\n",
    "    return None\n",
    "\n",
    "def fetch_fresh_data_direct():\n",
    "    \"\"\"Fetch fresh data by directly using the Google Play scraper library\"\"\"\n",
    "    # Import the scraper directly\n",
    "    from google_play_scraper import app as gp_app\n",
    "    from google_play_scraper import reviews as gp_reviews\n",
    "    from google_play_scraper.features.reviews import Sort\n",
    "    \n",
    "    print(f\"Fetching data directly using Google Play scraper...\")\n",
    "    \n",
    "    # Get app info first\n",
    "    try:\n",
    "        app_info = gp_app(app_id)\n",
    "        total_reported_reviews = app_info.get('reviews', 0)\n",
    "        print(f\"App info reports {total_reported_reviews} total reviews available\")\n",
    "        print(\"Note: This may be lower than actual available reviews\")\n",
    "        \n",
    "        # Use reported reviews as initial guide, but we'll keep fetching until no more are available\n",
    "        target_reviews = MAX_REVIEWS\n",
    "        print(f\"Will attempt to fetch up to {target_reviews} reviews\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting app info: {e}\")\n",
    "        target_reviews = MAX_REVIEWS\n",
    "    \n",
    "    all_reviews = []\n",
    "    continuation_token = None\n",
    "    retries = 0\n",
    "    max_retries = 3\n",
    "    \n",
    "    # Try multiple language/country combinations to maximize review collection\n",
    "    combinations = [\n",
    "        {\"lang\": \"en\", \"country\": \"us\"},  # Start with US English\n",
    "        {\"lang\": \"en\", \"country\": \"in\"},  # Then try Indian English\n",
    "        {\"lang\": \"hi\", \"country\": \"in\"},  # Hindi reviews from India\n",
    "        {\"lang\": \"en\", \"country\": \"gb\"}   # British English\n",
    "    ]\n",
    "    \n",
    "    # Loop through each combination\n",
    "    for combo in combinations:\n",
    "        print(f\"\\nTrying with language: {combo['lang']}, country: {combo['country']}\")\n",
    "        current_lang = combo['lang']\n",
    "        current_country = combo['country']\n",
    "        continuation_token = None  # Reset for each combination\n",
    "        combo_reviews = 0\n",
    "        \n",
    "        # Loop until we have enough reviews or no more are available\n",
    "        while len(all_reviews) < target_reviews:\n",
    "            try:\n",
    "                # Fetch batch of reviews (100 is the max per request)\n",
    "                batch_size = min(100, target_reviews - len(all_reviews))\n",
    "                if batch_size <= 0:\n",
    "                    break\n",
    "                    \n",
    "                print(f\"Fetching batch of {batch_size} reviews... (total so far: {len(all_reviews)})\")\n",
    "                result, continuation_token = gp_reviews(\n",
    "                    app_id=app_id,\n",
    "                    lang=current_lang,\n",
    "                    country=current_country,\n",
    "                    sort=Sort.NEWEST,\n",
    "                    count=batch_size,\n",
    "                    continuation_token=continuation_token\n",
    "                )\n",
    "                \n",
    "                # If no results, we're done with this combination\n",
    "                if not result:\n",
    "                    print(f\"No more reviews available for {current_lang}/{current_country}\")\n",
    "                    break\n",
    "                    \n",
    "                # Add results to our collection, avoiding duplicates\n",
    "                existing_ids = set(r.get('reviewId', '') for r in all_reviews)\n",
    "                new_reviews = [r for r in result if r.get('reviewId', '') not in existing_ids]\n",
    "                \n",
    "                if len(new_reviews) < len(result):\n",
    "                    print(f\"Filtered out {len(result) - len(new_reviews)} duplicate reviews\")\n",
    "                \n",
    "                all_reviews.extend(new_reviews)\n",
    "                combo_reviews += len(new_reviews)\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"Retrieved {len(new_reviews)} new reviews, total: {len(all_reviews)}/{target_reviews}\")\n",
    "                \n",
    "                # If no continuation token, we've reached the end for this combination\n",
    "                if not continuation_token:\n",
    "                    print(f\"No continuation token - reached the end of available reviews for {current_lang}/{current_country}\")\n",
    "                    break\n",
    "                \n",
    "                # Reset retries on successful fetch\n",
    "                retries = 0\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching reviews (attempt {retries}/{max_retries}): {e}\")\n",
    "                if retries >= max_retries:\n",
    "                    print(f\"Too many errors, moving to next language/country combination\")\n",
    "                    break\n",
    "                import time\n",
    "                time.sleep(2)  # Add a short delay before retrying\n",
    "        \n",
    "        print(f\"Completed fetching {combo_reviews} reviews for {current_lang}/{current_country}\")\n",
    "        \n",
    "        # If we've reached our target, we can stop\n",
    "        if len(all_reviews) >= target_reviews:\n",
    "            print(f\"Reached target of {target_reviews} reviews, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Completed fetching a total of {len(all_reviews)} reviews across all language/country combinations\")\n",
    "    \n",
    "    if not all_reviews:\n",
    "        print(\"No reviews retrieved. Using mock data instead.\")\n",
    "        return None\n",
    "    \n",
    "    # Transform review data to match our schema\n",
    "    transformed_reviews = []\n",
    "    for review in all_reviews:\n",
    "        # Basic schema transformation\n",
    "        transformed = {\n",
    "            \"review_id\": review.get(\"reviewId\", \"\"),\n",
    "            \"author\": review.get(\"userName\", \"\"),\n",
    "            \"rating\": review.get(\"score\", 0),\n",
    "            \"text\": review.get(\"content\", \"\"),\n",
    "            \"version\": review.get(\"reviewCreatedVersion\", \"\"),\n",
    "            \"thumbsUpCount\": review.get(\"thumbsUpCount\", 0),\n",
    "            \"replyContent\": review.get(\"replyContent\", None),\n",
    "            \"repliedAt\": review.get(\"repliedAt\", None)\n",
    "        }\n",
    "        \n",
    "        # Handle date fields\n",
    "        at_date = None\n",
    "        try:\n",
    "            # First try to use the 'at' field\n",
    "            if isinstance(review.get(\"at\"), datetime):\n",
    "                at_date = review[\"at\"]\n",
    "            elif review.get(\"at\"):\n",
    "                at_date = pd.to_datetime(review[\"at\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing 'at' field: {e}\")\n",
    "        \n",
    "        # Fallback to timeMillis if available\n",
    "        if at_date is None:\n",
    "            try:\n",
    "                if review.get(\"timeMillis\"):\n",
    "                    at_date = pd.to_datetime(review[\"timeMillis\"], unit='ms')\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing 'timeMillis' field: {e}\")\n",
    "        \n",
    "        transformed[\"date\"] = at_date\n",
    "        transformed[\"timestamp\"] = at_date  # Use the same value for timestamp\n",
    "        \n",
    "        transformed_reviews.append(transformed)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    reviews_df = pd.DataFrame(transformed_reviews)\n",
    "    \n",
    "    # Write the data to file\n",
    "    csv_path = RAW_DATA_PATH\n",
    "    reviews_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(f\"\\nSuccessfully processed {len(reviews_df)} reviews directly from Google Play\")\n",
    "    print(\"\\nFirst 5 rows of fetched data:\")\n",
    "    print(reviews_df.head().to_string())\n",
    "    \n",
    "    print(f\"Saved raw data to {RAW_DATA_PATH}\")\n",
    "    \n",
    "    return reviews_df\n",
    "\n",
    "def fetch_fresh_data():\n",
    "    \"\"\"Fetch fresh data from Google Play API or generate mock data\"\"\"\n",
    "    if USE_MOCK_DATA:\n",
    "        # Initialize the runner for mock data\n",
    "        runner = ReviewAnalysisRunner()\n",
    "        runner._initialize_modules()\n",
    "        \n",
    "        print(\"Using mock data source...\")\n",
    "        # Fetch mock reviews directly from the acquisition module\n",
    "        fresh_reviews_df = runner.acquisition.fetch_reviews(\n",
    "            app_id=app_id,\n",
    "            max_reviews=MAX_REVIEWS,\n",
    "            use_mock=True\n",
    "        )\n",
    "        \n",
    "        if fresh_reviews_df is not None and not fresh_reviews_df.empty:\n",
    "            print(f\"Successfully generated {len(fresh_reviews_df)} mock reviews\")\n",
    "            \n",
    "            # Display the first few rows of the mock data\n",
    "            print(\"\\nFirst 5 rows of mock data:\")\n",
    "            print(fresh_reviews_df.head().to_string())\n",
    "            \n",
    "            # Save to CSV\n",
    "            fresh_reviews_df.to_csv(RAW_DATA_PATH, index=False)\n",
    "            print(f\"Saved mock data to {RAW_DATA_PATH}\")\n",
    "            \n",
    "            return fresh_reviews_df\n",
    "        else:\n",
    "            print(\"Failed to generate mock reviews.\")\n",
    "            return None\n",
    "    else:\n",
    "        # For real data, use direct API access to avoid issues with the runner\n",
    "        return fetch_fresh_data_direct()\n",
    "\n",
    "# Main data acquisition logic\n",
    "print(\"\\n★ Data Acquisition Phase ★\")\n",
    "\n",
    "# Always respect FORCE_REFRESH flag\n",
    "if FORCE_REFRESH:\n",
    "    print(\"Force refresh requested, fetching fresh data...\")\n",
    "    reviews_df = fetch_fresh_data()\n",
    "else:\n",
    "    # Try to load existing data first\n",
    "    reviews_df = load_existing_data()\n",
    "    \n",
    "    # Only proceed to fetch new data if:\n",
    "    # 1. No existing data was found, or\n",
    "    # 2. We didn't get enough reviews \n",
    "    if reviews_df is None or len(reviews_df) < MAX_REVIEWS:\n",
    "        print(f\"Existing data not sufficient (have {len(reviews_df) if reviews_df is not None else 0}, need {MAX_REVIEWS})\")\n",
    "        print(\"Fetching fresh data...\")\n",
    "        reviews_df = fetch_fresh_data()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "if reviews_df is not None and not reviews_df.empty:\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {reviews_df.shape} (rows, columns)\")\n",
    "    print(f\"- Columns: {reviews_df.columns.tolist()}\")\n",
    "    print(f\"- Date range: {reviews_df['date'].min()} to {reviews_df['date'].max()}\")\n",
    "    print(f\"- Rating distribution:\\n{reviews_df['rating'].value_counts().sort_index()}\")\n",
    "    \n",
    "    # Check for nulls in important columns\n",
    "    null_counts = reviews_df[['review_id', 'date', 'rating', 'text']].isnull().sum()\n",
    "    print(f\"\\nNull values in key columns:\\n{null_counts}\")\n",
    "else:\n",
    "    print(\"No data available for processing. Please check your configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "This section performs initial cleaning of the raw data to prepare it for preprocessing:\n",
    "1. Removing duplicates\n",
    "2. Handling missing values\n",
    "3. Basic text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "Removed 0 duplicate reviews\n",
      "Dropped 0 rows with missing critical data\n",
      "Cleaning complete. Resulting dataset has 5000 rows\n",
      "\n",
      "Sample of cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15c7b93a-00a9-46ed-b5d0-ff21acee2c67</td>\n",
       "      <td>2025-05-07 18:18:21</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a6d3874-5be3-492b-954c-24390dfd2afe</td>\n",
       "      <td>2025-05-07 17:24:26</td>\n",
       "      <td>1</td>\n",
       "      <td>such an irritating app is this. you will open ...</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67f42ed4-4fb9-44f3-b548-023ee98eb488</td>\n",
       "      <td>2025-05-07 16:21:42</td>\n",
       "      <td>5</td>\n",
       "      <td>good nice</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dc8bd34d-d4ee-40fd-8f91-932f472d6d43</td>\n",
       "      <td>2025-05-07 16:18:56</td>\n",
       "      <td>1</td>\n",
       "      <td>app is not opening, please rectify</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ce29084-1e76-4846-8688-1819614b289f</td>\n",
       "      <td>2025-05-07 16:14:14</td>\n",
       "      <td>1</td>\n",
       "      <td>after upgrade its converted to worst app. and ...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              review_id                date  rating  \\\n",
       "0  15c7b93a-00a9-46ed-b5d0-ff21acee2c67 2025-05-07 18:18:21       5   \n",
       "1  2a6d3874-5be3-492b-954c-24390dfd2afe 2025-05-07 17:24:26       1   \n",
       "2  67f42ed4-4fb9-44f3-b548-023ee98eb488 2025-05-07 16:21:42       5   \n",
       "3  dc8bd34d-d4ee-40fd-8f91-932f472d6d43 2025-05-07 16:18:56       1   \n",
       "4  0ce29084-1e76-4846-8688-1819614b289f 2025-05-07 16:14:14       1   \n",
       "\n",
       "                                                text  text_length  \n",
       "0                                               good            4  \n",
       "1  such an irritating app is this. you will open ...          157  \n",
       "2                                          good nice            9  \n",
       "3                 app is not opening, please rectify           34  \n",
       "4  after upgrade its converted to worst app. and ...           75  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean the raw data to prepare for preprocessing\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to clean.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Cleaning data...\")\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # 1. Remove duplicates based on review_id\n",
    "    original_count = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=['review_id'])\n",
    "    print(f\"Removed {original_count - len(cleaned_df)} duplicate reviews\")\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    # For text: replace NaN with empty string\n",
    "    cleaned_df['text'] = cleaned_df['text'].fillna('')\n",
    "    \n",
    "    # For author: replace NaN with 'Anonymous'\n",
    "    cleaned_df['author'] = cleaned_df['author'].fillna('Anonymous')\n",
    "    \n",
    "    # For version: replace NaN with 'Unknown'\n",
    "    cleaned_df['version'] = cleaned_df['version'].fillna('Unknown')\n",
    "    \n",
    "    # 3. Basic text cleaning\n",
    "    # Remove excessive whitespace\n",
    "    cleaned_df['text'] = cleaned_df['text'].apply(lambda x: re.sub(r'\\s+', ' ', str(x).strip()))\n",
    "    \n",
    "    # Add text length as a feature\n",
    "    cleaned_df['text_length'] = cleaned_df['text'].apply(len)\n",
    "    \n",
    "    # Drop rows with missing critical data (review_id, date, rating)\n",
    "    original_count = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df.dropna(subset=['review_id', 'date', 'rating'])\n",
    "    print(f\"Dropped {original_count - len(cleaned_df)} rows with missing critical data\")\n",
    "    \n",
    "    # Ensure rating is numeric\n",
    "    cleaned_df['rating'] = pd.to_numeric(cleaned_df['rating'], errors='coerce')\n",
    "    \n",
    "    print(f\"Cleaning complete. Resulting dataset has {len(cleaned_df)} rows\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Clean the data\n",
    "cleaned_df = clean_data(reviews_df)\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "if cleaned_df is not None and not cleaned_df.empty:\n",
    "    print(\"\\nSample of cleaned data:\")\n",
    "    display(cleaned_df[['review_id', 'date', 'rating', 'text', 'text_length']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section adds new features to the dataset to enhance analysis:\n",
    "1. Sentiment analysis\n",
    "2. Text complexity metrics\n",
    "3. Time-based features\n",
    "4. Additional categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features...\n",
      "Adding text preprocessing columns...\n",
      "Cleaning and normalizing review text...\n",
      "Text preprocessing complete.\n",
      "Feature engineering complete. Added 11 new features\n",
      "\n",
      "New features added:\n",
      "avg_word_length, cleaned_text, day_of_week, is_weekend, length_category, major_version, month, normalized_text, sentiment, word_count, year\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "negative    2797\n",
      "positive    2037\n",
      "neutral      166\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length category distribution:\n",
      "length_category\n",
      "very_short    3339\n",
      "short         1314\n",
      "medium         336\n",
      "long            11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Major version distribution (top 5):\n",
      "major_version\n",
      "10         2720\n",
      "9          1481\n",
      "Unknown     783\n",
      "8            14\n",
      "6             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Text preprocessing sample:\n",
      "\n",
      "Original: good\n",
      "Cleaned:  good\n",
      "Normalized: good\n",
      "\n",
      "Original: such an irritating app is this. you will open to do some important work and everytime it will redirect you to update some other app which is already updated.\n",
      "Cleaned:  such an irritating app is this you will open to do some important work and everytime it will redirect you to update some other app which is already updated\n",
      "Normalized: irritating app open important work everytime redirect update app already updated\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"Add new features to the dataset to enhance analysis\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data for feature engineering.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Adding features...\")\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    # 1. Simple sentiment analysis based on rating\n",
    "    def get_sentiment_from_rating(rating):\n",
    "        if rating >= 4:\n",
    "            return 'positive'\n",
    "        elif rating <= 2:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    enhanced_df['sentiment'] = enhanced_df['rating'].apply(get_sentiment_from_rating)\n",
    "    \n",
    "    # 2. Time-based features\n",
    "    enhanced_df['year'] = enhanced_df['date'].dt.year\n",
    "    enhanced_df['month'] = enhanced_df['date'].dt.month\n",
    "    enhanced_df['day_of_week'] = enhanced_df['date'].dt.day_name()\n",
    "    enhanced_df['is_weekend'] = enhanced_df['day_of_week'].isin(['Saturday', 'Sunday'])\n",
    "    \n",
    "    # 3. Text complexity metrics\n",
    "    # Simple metrics: word count and average word length\n",
    "    enhanced_df['word_count'] = enhanced_df['text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    def avg_word_length(text):\n",
    "        words = str(text).split()\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    enhanced_df['avg_word_length'] = enhanced_df['text'].apply(avg_word_length)\n",
    "    \n",
    "    # 4. Categorize reviews by text length\n",
    "    def categorize_length(length):\n",
    "        if length == 0:\n",
    "            return 'empty'\n",
    "        elif length < 50:\n",
    "            return 'very_short'\n",
    "        elif length < 200:\n",
    "            return 'short'\n",
    "        elif length < 500:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'long'\n",
    "    \n",
    "    enhanced_df['length_category'] = enhanced_df['text_length'].apply(categorize_length)\n",
    "    \n",
    "    # 5. Version-based features\n",
    "    # Extract major version (e.g., '7.2.0' -> '7')\n",
    "    enhanced_df['major_version'] = enhanced_df['version'].apply(\n",
    "        lambda x: x.split('.')[0] if isinstance(x, str) and '.' in x else 'Unknown'\n",
    "    )\n",
    "    \n",
    "    # 6. Add text preprocessing columns\n",
    "    print(\"Adding text preprocessing columns...\")\n",
    "    try:\n",
    "        # Import NLP preprocessor\n",
    "        sys.path.insert(0, project_root)\n",
    "        from src.modules.preprocessing.nlp_preprocessor import NLPPreprocessor\n",
    "        \n",
    "        # Initialize the preprocessor\n",
    "        preprocessor = NLPPreprocessor({\"enable_lemmatization\": True})\n",
    "        if not hasattr(preprocessor, 'is_initialized') or not preprocessor.is_initialized:\n",
    "            preprocessor.initialize()\n",
    "        \n",
    "        # Apply preprocessing to text column\n",
    "        print(\"Cleaning and normalizing review text...\")\n",
    "        enhanced_df['cleaned_text'] = enhanced_df['text'].apply(\n",
    "            lambda x: preprocessor.clean_text(str(x)) if pd.notna(x) else \"\")\n",
    "        \n",
    "        enhanced_df['normalized_text'] = enhanced_df['cleaned_text'].apply(\n",
    "            lambda x: preprocessor.normalize_text(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        print(\"Text preprocessing complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text preprocessing: {e}\")\n",
    "        print(\"Falling back to basic text cleaning...\")\n",
    "        \n",
    "        # Basic fallback preprocessing if the advanced module fails\n",
    "        import re\n",
    "        import nltk\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            from nltk.corpus import stopwords\n",
    "            stopwords_list = set(stopwords.words('english'))\n",
    "        except:\n",
    "            # If NLTK is not available, use a small set of common stopwords\n",
    "            stopwords_list = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', \n",
    "                              'was', 'were', 'to', 'of', 'in', 'for', 'with'}\n",
    "        \n",
    "        # Simple cleaning function\n",
    "        def basic_clean(text):\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            # Remove special characters and punctuation\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Remove numbers\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "            \n",
    "        # Simple stopword removal\n",
    "        def remove_stopwords(text):\n",
    "            return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "        \n",
    "        # Apply basic cleaning\n",
    "        enhanced_df['cleaned_text'] = enhanced_df['text'].apply(\n",
    "            lambda x: basic_clean(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        # Apply stopword removal for normalization\n",
    "        enhanced_df['normalized_text'] = enhanced_df['cleaned_text'].apply(\n",
    "            lambda x: remove_stopwords(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        print(\"Basic text preprocessing complete.\")\n",
    "    \n",
    "    print(f\"Feature engineering complete. Added {len(enhanced_df.columns) - len(df.columns)} new features\")\n",
    "    return enhanced_df\n",
    "\n",
    "# Add features\n",
    "enhanced_df = add_features(cleaned_df)\n",
    "\n",
    "# Display a summary of the new features\n",
    "if enhanced_df is not None and not enhanced_df.empty:\n",
    "    print(\"\\nNew features added:\")\n",
    "    new_features = set(enhanced_df.columns) - set(cleaned_df.columns)\n",
    "    print(', '.join(sorted(new_features)))\n",
    "    \n",
    "    # Display feature statistics\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(enhanced_df['sentiment'].value_counts())\n",
    "    \n",
    "    print(\"\\nLength category distribution:\")\n",
    "    print(enhanced_df['length_category'].value_counts())\n",
    "    \n",
    "    print(\"\\nMajor version distribution (top 5):\")\n",
    "    print(enhanced_df['major_version'].value_counts().head())\n",
    "    \n",
    "    # Display sample of text preprocessing\n",
    "    print(\"\\nText preprocessing sample:\")\n",
    "    sample_df = enhanced_df[['text', 'cleaned_text', 'normalized_text']].head(2)\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nOriginal: {row['text']}\")\n",
    "        print(f\"Cleaned:  {row['cleaned_text']}\")\n",
    "        print(f\"Normalized: {row['normalized_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Processing and Export\n",
    "\n",
    "This section performs the final processing steps and exports the data to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing final processing...\n",
      "Processed data exported to /Users/dipesh/Local-Projects/indigo-reviews-ai/data/processed/processed_reviews.csv\n",
      "\n",
      "Final Dataset Summary:\n",
      "- Shape: (5000, 22)\n",
      "- Memory usage: 786.26 KB\n",
      "- Column list: review_id, author, rating, text, version, thumbsUpCount, replyContent, repliedAt, date, timestamp, text_length, sentiment, year, month, day_of_week, is_weekend, word_count, avg_word_length, length_category, major_version, cleaned_text, normalized_text\n",
      "\n",
      "Sample of processed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>length_category</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15c7b93a-00a9-46ed-b5d0-ff21acee2c67</td>\n",
       "      <td>2025-05-07 18:18:21</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>very_short</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a6d3874-5be3-492b-954c-24390dfd2afe</td>\n",
       "      <td>2025-05-07 17:24:26</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>such an irritating app is this. you will open ...</td>\n",
       "      <td>short</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67f42ed4-4fb9-44f3-b548-023ee98eb488</td>\n",
       "      <td>2025-05-07 16:21:42</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good nice</td>\n",
       "      <td>very_short</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dc8bd34d-d4ee-40fd-8f91-932f472d6d43</td>\n",
       "      <td>2025-05-07 16:18:56</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is not opening, please rectify</td>\n",
       "      <td>very_short</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ce29084-1e76-4846-8688-1819614b289f</td>\n",
       "      <td>2025-05-07 16:14:14</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>after upgrade its converted to worst app. and ...</td>\n",
       "      <td>short</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              review_id                date  rating sentiment  \\\n",
       "0  15c7b93a-00a9-46ed-b5d0-ff21acee2c67 2025-05-07 18:18:21       5  positive   \n",
       "1  2a6d3874-5be3-492b-954c-24390dfd2afe 2025-05-07 17:24:26       1  negative   \n",
       "2  67f42ed4-4fb9-44f3-b548-023ee98eb488 2025-05-07 16:21:42       5  positive   \n",
       "3  dc8bd34d-d4ee-40fd-8f91-932f472d6d43 2025-05-07 16:18:56       1  negative   \n",
       "4  0ce29084-1e76-4846-8688-1819614b289f 2025-05-07 16:14:14       1  negative   \n",
       "\n",
       "                                                text length_category  \\\n",
       "0                                               good      very_short   \n",
       "1  such an irritating app is this. you will open ...           short   \n",
       "2                                          good nice      very_short   \n",
       "3                 app is not opening, please rectify      very_short   \n",
       "4  after upgrade its converted to worst app. and ...           short   \n",
       "\n",
       "   word_count  \n",
       "0           1  \n",
       "1          29  \n",
       "2           2  \n",
       "3           6  \n",
       "4          13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def finalize_and_export(df):\n",
    "    \"\"\"Perform final processing and export the data\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to export.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Performing final processing...\")\n",
    "    final_df = df.copy()\n",
    "    \n",
    "    # Sort by date (newest first)\n",
    "    final_df = final_df.sort_values('date', ascending=False)\n",
    "    \n",
    "    # Reset index\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    # Export to CSV\n",
    "    final_df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "    print(f\"Processed data exported to {PROCESSED_DATA_PATH}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Finalize and export\n",
    "final_df = finalize_and_export(enhanced_df)\n",
    "\n",
    "# Display final dataset info\n",
    "if final_df is not None and not final_df.empty:\n",
    "    print(\"\\nFinal Dataset Summary:\")\n",
    "    print(f\"- Shape: {final_df.shape}\")\n",
    "    print(f\"- Memory usage: {final_df.memory_usage().sum() / 1024:.2f} KB\")\n",
    "    print(f\"- Column list: {', '.join(final_df.columns.tolist())}\")\n",
    "    \n",
    "    # Display sample rows\n",
    "    print(\"\\nSample of processed data:\")\n",
    "    sample_columns = ['review_id', 'date', 'rating', 'sentiment', 'text', 'length_category', 'word_count']\n",
    "    display(final_df[sample_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has performed the following steps:\n",
    "\n",
    "1. Acquired data from the appropriate source (API, CSV, or mock)\n",
    "2. Cleaned the data by handling duplicates and missing values\n",
    "3. Added features for enhanced analysis\n",
    "4. Exported the processed data to `processed_reviews.csv`\n",
    "\n",
    "The processed data can now be used by other notebooks for analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
