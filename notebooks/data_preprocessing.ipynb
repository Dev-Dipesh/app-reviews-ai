{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook\n",
    "\n",
    "This notebook handles the complete lifecycle of data acquisition, cleaning, and preprocessing. It can:\n",
    "\n",
    "1. Fetch data from Google Play Store API (real data)\n",
    "2. Load data from existing CSV files (cached data)\n",
    "3. Generate mock data for testing purposes\n",
    "\n",
    "The output of this notebook is a clean, processed dataset stored in `processed_reviews.csv` that can be used by other notebooks for analysis and visualization.\n",
    "\n",
    "**Note:** You can control notebook behavior with these environment variables:\n",
    "- `USE_MOCK_DATA=true` - Use mock data instead of real API data\n",
    "- `FORCE_REFRESH=true` - Force fetching fresh data even if existing data is available\n",
    "- `MAX_REVIEWS=100` - Set the maximum number of reviews to fetch/process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/dipesh/Local-Projects/indigo-reviews-ai to Python path\n",
      "DEBUG: Applying APP_ID from environment: 'in.goindigo.android'\n",
      "Configuration:\n",
      "\n",
      "- App ID: in.goindigo.android\n",
      "- Max Reviews: 10000\n",
      "- Use Mock Data: False\n",
      "- Force Refresh: False\n",
      "- Raw Data Path: /Users/dipesh/Local-Projects/indigo-reviews-ai/data/reviews.csv\n",
      "- Processed Data Path: /Users/dipesh/Local-Projects/indigo-reviews-ai/data/processed/processed_reviews.csv\n",
      "\n",
      "Environment variables:\n",
      "- MAX_REVIEWS: 10000\n",
      "- USE_MOCK_DATA: false\n",
      "- FORCE_REFRESH: false\n",
      "- APP_ID: in.goindigo.android\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import importlib\n",
    "import subprocess  # Add subprocess import\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Import project modules\n",
    "from src.runner import ReviewAnalysisRunner\n",
    "from src.modules.acquisition import google_play\n",
    "from src.modules.storage import file_storage\n",
    "from src.modules.preprocessing import nlp_preprocessor\n",
    "\n",
    "# Force reload modules to get latest changes\n",
    "importlib.reload(google_play)\n",
    "importlib.reload(file_storage)\n",
    "importlib.reload(nlp_preprocessor)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration and paths\n",
    "DATA_DIR = os.path.join(project_root, 'data')  # Updated to use /data instead of /src/data\n",
    "RAW_DATA_PATH = os.path.join(DATA_DIR, 'reviews.csv')\n",
    "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, 'processed', 'processed_reviews.csv')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(PROCESSED_DATA_PATH), exist_ok=True)\n",
    "\n",
    "# Environment variables\n",
    "USE_MOCK_DATA = os.environ.get('USE_MOCK_DATA', 'false').lower() in ('true', '1', 'yes', 'y')\n",
    "FORCE_REFRESH = os.environ.get('FORCE_REFRESH', 'false').lower() in ('true', '1', 'yes', 'y')\n",
    "MAX_REVIEWS = int(os.environ.get('MAX_REVIEWS', '100'))\n",
    "\n",
    "# Extract APP_ID properly (removing any comments)\n",
    "app_id_env = os.environ.get(\"APP_ID\", \"in.goindigo.android\")\n",
    "app_id = app_id_env.split('#')[0].strip()  # Remove any comments and whitespace\n",
    "\n",
    "print(f\"Configuration:\\n\")\n",
    "print(f\"- App ID: {app_id}\")\n",
    "print(f\"- Max Reviews: {MAX_REVIEWS}\")\n",
    "print(f\"- Use Mock Data: {USE_MOCK_DATA}\")\n",
    "print(f\"- Force Refresh: {FORCE_REFRESH}\")\n",
    "print(f\"- Raw Data Path: {RAW_DATA_PATH}\")\n",
    "print(f\"- Processed Data Path: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# Debug: Show environment variables as seen by Python\n",
    "print(\"\\nEnvironment variables:\")\n",
    "for env_var in ['MAX_REVIEWS', 'USE_MOCK_DATA', 'FORCE_REFRESH', 'APP_ID']:\n",
    "    print(f\"- {env_var}: {os.environ.get(env_var, 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "★ Data Acquisition Phase ★\n",
      "Successfully loaded 10000 reviews from /Users/dipesh/Local-Projects/indigo-reviews-ai/data/reviews.csv\n",
      "\n",
      "First 5 rows of loaded data:\n",
      "                              review_id                    author  rating                                                                                                                                                                                          text version  thumbsUpCount replyContent repliedAt                date            timestamp\n",
      "0  0033822f-bb10-43a1-ad24-a9504001edba          Indravadan Patel       5                                                                                                                                                                                     very nice   7.2.0              0          NaN       NaN 2025-05-07 12:19:43  2025-05-07 12:19:43\n",
      "1  f15ac7dd-411c-4455-bba5-f08ea195fbf6              Aditya Dewan       1  very bad interface, booked a normal fare ticket even after selecting student fare, bc student fare had to be chosen twice. I paid extra and have 15 kg luggage allowance and can't change it   7.2.4              0          NaN       NaN 2025-05-07 09:17:44  2025-05-07 09:17:44\n",
      "2  b6c32819-9275-419a-8ed5-268cf123ca7d            Osman Mohammed       5                                                                                                                                                                       good update information   7.2.4              0          NaN       NaN 2025-05-07 07:59:32  2025-05-07 07:59:32\n",
      "3  ab99aad1-b18e-47e7-b52f-1436daca2da8              Sahil Sharma       1                                                                                                                                                                                sucks big time     NaN              0          NaN       NaN 2025-05-07 00:23:10  2025-05-07 00:23:10\n",
      "4  c5e97ce3-743f-4028-9b01-8079df9fa724  Unproductive Mail (Zain)       1                                                                                                                  worst app and worst website, its like some child has developed this website.   7.2.4              0          NaN       NaN 2025-05-06 23:16:04  2025-05-06 23:16:04\n",
      "\n",
      "Dataset Overview:\n",
      "- Shape: (10000, 10) (rows, columns)\n",
      "- Columns: ['review_id', 'author', 'rating', 'text', 'version', 'thumbsUpCount', 'replyContent', 'repliedAt', 'date', 'timestamp']\n",
      "- Date range: 2023-08-27 12:35:07 to 2025-05-07 12:19:43\n",
      "- Rating distribution:\n",
      "rating\n",
      "1    3688\n",
      "2     549\n",
      "3     539\n",
      "4     822\n",
      "5    4402\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Null values in key columns:\n",
      "review_id    0\n",
      "date         0\n",
      "rating       0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_existing_data():\n",
    "    \"\"\"Attempt to load existing data from reviews.csv\"\"\"\n",
    "    if os.path.exists(RAW_DATA_PATH):\n",
    "        try:\n",
    "            df = pd.read_csv(RAW_DATA_PATH)\n",
    "            # Convert date column to datetime\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            print(f\"Successfully loaded {len(df)} reviews from {RAW_DATA_PATH}\")\n",
    "            \n",
    "            # Display the first few rows of the loaded data\n",
    "            print(\"\\nFirst 5 rows of loaded data:\")\n",
    "            print(df.head().to_string())\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing data: {e}\")\n",
    "    print(f\"No existing data found at {RAW_DATA_PATH}\")\n",
    "    return None\n",
    "\n",
    "def fetch_fresh_data_direct():\n",
    "    \"\"\"Fetch fresh data by directly using the Google Play scraper library\"\"\"\n",
    "    # Import the scraper directly\n",
    "    from google_play_scraper import app as gp_app\n",
    "    from google_play_scraper import reviews as gp_reviews\n",
    "    from google_play_scraper.features.reviews import Sort\n",
    "    \n",
    "    print(f\"Fetching data directly using Google Play scraper...\")\n",
    "    \n",
    "    # Get app info first\n",
    "    try:\n",
    "        app_info = gp_app(app_id)\n",
    "        total_reported_reviews = app_info.get('reviews', 0)\n",
    "        print(f\"App info reports {total_reported_reviews} total reviews available\")\n",
    "        print(\"Note: This may be lower than actual available reviews\")\n",
    "        \n",
    "        # Use reported reviews as initial guide, but we'll keep fetching until no more are available\n",
    "        target_reviews = MAX_REVIEWS\n",
    "        print(f\"Will attempt to fetch up to {target_reviews} reviews\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting app info: {e}\")\n",
    "        target_reviews = MAX_REVIEWS\n",
    "    \n",
    "    all_reviews = []\n",
    "    continuation_token = None\n",
    "    retries = 0\n",
    "    max_retries = 3\n",
    "    \n",
    "    # Try multiple language/country combinations to maximize review collection\n",
    "    combinations = [\n",
    "        {\"lang\": \"en\", \"country\": \"us\"},  # Start with US English\n",
    "        {\"lang\": \"en\", \"country\": \"in\"},  # Then try Indian English\n",
    "        {\"lang\": \"hi\", \"country\": \"in\"},  # Hindi reviews from India\n",
    "        {\"lang\": \"en\", \"country\": \"gb\"}   # British English\n",
    "    ]\n",
    "    \n",
    "    # Loop through each combination\n",
    "    for combo in combinations:\n",
    "        print(f\"\\nTrying with language: {combo['lang']}, country: {combo['country']}\")\n",
    "        current_lang = combo['lang']\n",
    "        current_country = combo['country']\n",
    "        continuation_token = None  # Reset for each combination\n",
    "        combo_reviews = 0\n",
    "        \n",
    "        # Loop until we have enough reviews or no more are available\n",
    "        while len(all_reviews) < target_reviews:\n",
    "            try:\n",
    "                # Fetch batch of reviews (100 is the max per request)\n",
    "                batch_size = min(100, target_reviews - len(all_reviews))\n",
    "                if batch_size <= 0:\n",
    "                    break\n",
    "                    \n",
    "                print(f\"Fetching batch of {batch_size} reviews... (total so far: {len(all_reviews)})\")\n",
    "                result, continuation_token = gp_reviews(\n",
    "                    app_id=app_id,\n",
    "                    lang=current_lang,\n",
    "                    country=current_country,\n",
    "                    sort=Sort.NEWEST,\n",
    "                    count=batch_size,\n",
    "                    continuation_token=continuation_token\n",
    "                )\n",
    "                \n",
    "                # If no results, we're done with this combination\n",
    "                if not result:\n",
    "                    print(f\"No more reviews available for {current_lang}/{current_country}\")\n",
    "                    break\n",
    "                    \n",
    "                # Add results to our collection, avoiding duplicates\n",
    "                existing_ids = set(r.get('reviewId', '') for r in all_reviews)\n",
    "                new_reviews = [r for r in result if r.get('reviewId', '') not in existing_ids]\n",
    "                \n",
    "                if len(new_reviews) < len(result):\n",
    "                    print(f\"Filtered out {len(result) - len(new_reviews)} duplicate reviews\")\n",
    "                \n",
    "                all_reviews.extend(new_reviews)\n",
    "                combo_reviews += len(new_reviews)\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"Retrieved {len(new_reviews)} new reviews, total: {len(all_reviews)}/{target_reviews}\")\n",
    "                \n",
    "                # If no continuation token, we've reached the end for this combination\n",
    "                if not continuation_token:\n",
    "                    print(f\"No continuation token - reached the end of available reviews for {current_lang}/{current_country}\")\n",
    "                    break\n",
    "                \n",
    "                # Reset retries on successful fetch\n",
    "                retries = 0\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching reviews (attempt {retries}/{max_retries}): {e}\")\n",
    "                if retries >= max_retries:\n",
    "                    print(f\"Too many errors, moving to next language/country combination\")\n",
    "                    break\n",
    "                import time\n",
    "                time.sleep(2)  # Add a short delay before retrying\n",
    "        \n",
    "        print(f\"Completed fetching {combo_reviews} reviews for {current_lang}/{current_country}\")\n",
    "        \n",
    "        # If we've reached our target, we can stop\n",
    "        if len(all_reviews) >= target_reviews:\n",
    "            print(f\"Reached target of {target_reviews} reviews, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Completed fetching a total of {len(all_reviews)} reviews across all language/country combinations\")\n",
    "    \n",
    "    if not all_reviews:\n",
    "        print(\"No reviews retrieved. Using mock data instead.\")\n",
    "        return None\n",
    "    \n",
    "    # Transform review data to match our schema\n",
    "    transformed_reviews = []\n",
    "    for review in all_reviews:\n",
    "        # Basic schema transformation\n",
    "        transformed = {\n",
    "            \"review_id\": review.get(\"reviewId\", \"\"),\n",
    "            \"author\": review.get(\"userName\", \"\"),\n",
    "            \"rating\": review.get(\"score\", 0),\n",
    "            \"text\": review.get(\"content\", \"\"),\n",
    "            \"version\": review.get(\"reviewCreatedVersion\", \"\"),\n",
    "            \"thumbsUpCount\": review.get(\"thumbsUpCount\", 0),\n",
    "            \"replyContent\": review.get(\"replyContent\", None),\n",
    "            \"repliedAt\": review.get(\"repliedAt\", None)\n",
    "        }\n",
    "        \n",
    "        # Handle date fields\n",
    "        at_date = None\n",
    "        try:\n",
    "            # First try to use the 'at' field\n",
    "            if isinstance(review.get(\"at\"), datetime):\n",
    "                at_date = review[\"at\"]\n",
    "            elif review.get(\"at\"):\n",
    "                at_date = pd.to_datetime(review[\"at\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing 'at' field: {e}\")\n",
    "        \n",
    "        # Fallback to timeMillis if available\n",
    "        if at_date is None:\n",
    "            try:\n",
    "                if review.get(\"timeMillis\"):\n",
    "                    at_date = pd.to_datetime(review[\"timeMillis\"], unit='ms')\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing 'timeMillis' field: {e}\")\n",
    "        \n",
    "        transformed[\"date\"] = at_date\n",
    "        transformed[\"timestamp\"] = at_date  # Use the same value for timestamp\n",
    "        \n",
    "        transformed_reviews.append(transformed)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    reviews_df = pd.DataFrame(transformed_reviews)\n",
    "    \n",
    "    # Write the data to file\n",
    "    csv_path = RAW_DATA_PATH\n",
    "    reviews_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(f\"\\nSuccessfully processed {len(reviews_df)} reviews directly from Google Play\")\n",
    "    print(\"\\nFirst 5 rows of fetched data:\")\n",
    "    print(reviews_df.head().to_string())\n",
    "    \n",
    "    print(f\"Saved raw data to {RAW_DATA_PATH}\")\n",
    "    \n",
    "    return reviews_df\n",
    "\n",
    "def fetch_fresh_data():\n",
    "    \"\"\"Fetch fresh data from Google Play API or generate mock data\"\"\"\n",
    "    if USE_MOCK_DATA:\n",
    "        # Initialize the runner for mock data\n",
    "        runner = ReviewAnalysisRunner()\n",
    "        runner._initialize_modules()\n",
    "        \n",
    "        print(\"Using mock data source...\")\n",
    "        # Fetch mock reviews directly from the acquisition module\n",
    "        fresh_reviews_df = runner.acquisition.fetch_reviews(\n",
    "            app_id=app_id,\n",
    "            max_reviews=MAX_REVIEWS,\n",
    "            use_mock=True\n",
    "        )\n",
    "        \n",
    "        if fresh_reviews_df is not None and not fresh_reviews_df.empty:\n",
    "            print(f\"Successfully generated {len(fresh_reviews_df)} mock reviews\")\n",
    "            \n",
    "            # Display the first few rows of the mock data\n",
    "            print(\"\\nFirst 5 rows of mock data:\")\n",
    "            print(fresh_reviews_df.head().to_string())\n",
    "            \n",
    "            # Save to CSV\n",
    "            fresh_reviews_df.to_csv(RAW_DATA_PATH, index=False)\n",
    "            print(f\"Saved mock data to {RAW_DATA_PATH}\")\n",
    "            \n",
    "            return fresh_reviews_df\n",
    "        else:\n",
    "            print(\"Failed to generate mock reviews.\")\n",
    "            return None\n",
    "    else:\n",
    "        # For real data, use direct API access to avoid issues with the runner\n",
    "        return fetch_fresh_data_direct()\n",
    "\n",
    "# Main data acquisition logic\n",
    "print(\"\\n★ Data Acquisition Phase ★\")\n",
    "\n",
    "# Always respect FORCE_REFRESH flag\n",
    "if FORCE_REFRESH:\n",
    "    print(\"Force refresh requested, fetching fresh data...\")\n",
    "    reviews_df = fetch_fresh_data()\n",
    "else:\n",
    "    # Try to load existing data first\n",
    "    reviews_df = load_existing_data()\n",
    "    \n",
    "    # Only proceed to fetch new data if:\n",
    "    # 1. No existing data was found, or\n",
    "    # 2. We didn't get enough reviews \n",
    "    if reviews_df is None or len(reviews_df) < MAX_REVIEWS:\n",
    "        print(f\"Existing data not sufficient (have {len(reviews_df) if reviews_df is not None else 0}, need {MAX_REVIEWS})\")\n",
    "        print(\"Fetching fresh data...\")\n",
    "        reviews_df = fetch_fresh_data()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "if reviews_df is not None and not reviews_df.empty:\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {reviews_df.shape} (rows, columns)\")\n",
    "    print(f\"- Columns: {reviews_df.columns.tolist()}\")\n",
    "    print(f\"- Date range: {reviews_df['date'].min()} to {reviews_df['date'].max()}\")\n",
    "    print(f\"- Rating distribution:\\n{reviews_df['rating'].value_counts().sort_index()}\")\n",
    "    \n",
    "    # Check for nulls in important columns\n",
    "    null_counts = reviews_df[['review_id', 'date', 'rating', 'text']].isnull().sum()\n",
    "    print(f\"\\nNull values in key columns:\\n{null_counts}\")\n",
    "else:\n",
    "    print(\"No data available for processing. Please check your configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "This section performs initial cleaning of the raw data to prepare it for preprocessing:\n",
    "1. Removing duplicates\n",
    "2. Handling missing values\n",
    "3. Basic text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "Removed 0 duplicate reviews\n",
      "Dropped 0 rows with missing critical data\n",
      "Cleaning complete. Resulting dataset has 10000 rows\n",
      "\n",
      "Sample of cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0033822f-bb10-43a1-ad24-a9504001edba</td>\n",
       "      <td>2025-05-07 12:19:43</td>\n",
       "      <td>5</td>\n",
       "      <td>very nice</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f15ac7dd-411c-4455-bba5-f08ea195fbf6</td>\n",
       "      <td>2025-05-07 09:17:44</td>\n",
       "      <td>1</td>\n",
       "      <td>very bad interface, booked a normal fare ticke...</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6c32819-9275-419a-8ed5-268cf123ca7d</td>\n",
       "      <td>2025-05-07 07:59:32</td>\n",
       "      <td>5</td>\n",
       "      <td>good update information</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab99aad1-b18e-47e7-b52f-1436daca2da8</td>\n",
       "      <td>2025-05-07 00:23:10</td>\n",
       "      <td>1</td>\n",
       "      <td>sucks big time</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c5e97ce3-743f-4028-9b01-8079df9fa724</td>\n",
       "      <td>2025-05-06 23:16:04</td>\n",
       "      <td>1</td>\n",
       "      <td>worst app and worst website, its like some chi...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              review_id                date  rating  \\\n",
       "0  0033822f-bb10-43a1-ad24-a9504001edba 2025-05-07 12:19:43       5   \n",
       "1  f15ac7dd-411c-4455-bba5-f08ea195fbf6 2025-05-07 09:17:44       1   \n",
       "2  b6c32819-9275-419a-8ed5-268cf123ca7d 2025-05-07 07:59:32       5   \n",
       "3  ab99aad1-b18e-47e7-b52f-1436daca2da8 2025-05-07 00:23:10       1   \n",
       "4  c5e97ce3-743f-4028-9b01-8079df9fa724 2025-05-06 23:16:04       1   \n",
       "\n",
       "                                                text  text_length  \n",
       "0                                          very nice            9  \n",
       "1  very bad interface, booked a normal fare ticke...          188  \n",
       "2                            good update information           23  \n",
       "3                                     sucks big time           14  \n",
       "4  worst app and worst website, its like some chi...           76  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean the raw data to prepare for preprocessing\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to clean.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Cleaning data...\")\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # 1. Remove duplicates based on review_id\n",
    "    original_count = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=['review_id'])\n",
    "    print(f\"Removed {original_count - len(cleaned_df)} duplicate reviews\")\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    # For text: replace NaN with empty string\n",
    "    cleaned_df['text'] = cleaned_df['text'].fillna('')\n",
    "    \n",
    "    # For author: replace NaN with 'Anonymous'\n",
    "    cleaned_df['author'] = cleaned_df['author'].fillna('Anonymous')\n",
    "    \n",
    "    # For version: replace NaN with 'Unknown'\n",
    "    cleaned_df['version'] = cleaned_df['version'].fillna('Unknown')\n",
    "    \n",
    "    # 3. Basic text cleaning\n",
    "    # Remove excessive whitespace\n",
    "    cleaned_df['text'] = cleaned_df['text'].apply(lambda x: re.sub(r'\\s+', ' ', str(x).strip()))\n",
    "    \n",
    "    # Add text length as a feature\n",
    "    cleaned_df['text_length'] = cleaned_df['text'].apply(len)\n",
    "    \n",
    "    # Drop rows with missing critical data (review_id, date, rating)\n",
    "    original_count = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df.dropna(subset=['review_id', 'date', 'rating'])\n",
    "    print(f\"Dropped {original_count - len(cleaned_df)} rows with missing critical data\")\n",
    "    \n",
    "    # Ensure rating is numeric\n",
    "    cleaned_df['rating'] = pd.to_numeric(cleaned_df['rating'], errors='coerce')\n",
    "    \n",
    "    print(f\"Cleaning complete. Resulting dataset has {len(cleaned_df)} rows\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Clean the data\n",
    "cleaned_df = clean_data(reviews_df)\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "if cleaned_df is not None and not cleaned_df.empty:\n",
    "    print(\"\\nSample of cleaned data:\")\n",
    "    display(cleaned_df[['review_id', 'date', 'rating', 'text', 'text_length']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section adds new features to the dataset to enhance analysis:\n",
    "1. Sentiment analysis\n",
    "2. Text complexity metrics\n",
    "3. Time-based features\n",
    "4. Additional categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features...\n",
      "Adding text preprocessing columns...\n",
      "Cleaning and normalizing review text...\n",
      "Text preprocessing complete.\n",
      "Feature engineering complete. Added 11 new features\n",
      "\n",
      "New features added:\n",
      "avg_word_length, cleaned_text, day_of_week, is_weekend, length_category, major_version, month, normalized_text, sentiment, word_count, year\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    5224\n",
      "negative    4237\n",
      "neutral      539\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length category distribution:\n",
      "length_category\n",
      "very_short    6597\n",
      "short         2487\n",
      "medium         886\n",
      "long            30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Major version distribution (top 5):\n",
      "major_version\n",
      "6          4586\n",
      "5          3001\n",
      "7          1418\n",
      "Unknown     990\n",
      "3             3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Text preprocessing sample:\n",
      "\n",
      "Original: very nice\n",
      "Cleaned:  very nice\n",
      "Normalized: nice\n",
      "\n",
      "Original: very bad interface, booked a normal fare ticket even after selecting student fare, bc student fare had to be chosen twice. I paid extra and have 15 kg luggage allowance and can't change it\n",
      "Cleaned:  very bad interface booked a normal fare ticket even after selecting student fare bc student fare had to be chosen twice i paid extra and have kg luggage allowance and cant change it\n",
      "Normalized: bad interface booked normal fare ticket even selecting student fare student fare chosen twice paid extra luggage allowance cant change\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"Add new features to the dataset to enhance analysis\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data for feature engineering.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Adding features...\")\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    # 1. Simple sentiment analysis based on rating\n",
    "    def get_sentiment_from_rating(rating):\n",
    "        if rating >= 4:\n",
    "            return 'positive'\n",
    "        elif rating <= 2:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    enhanced_df['sentiment'] = enhanced_df['rating'].apply(get_sentiment_from_rating)\n",
    "    \n",
    "    # 2. Time-based features\n",
    "    enhanced_df['year'] = enhanced_df['date'].dt.year\n",
    "    enhanced_df['month'] = enhanced_df['date'].dt.month\n",
    "    enhanced_df['day_of_week'] = enhanced_df['date'].dt.day_name()\n",
    "    enhanced_df['is_weekend'] = enhanced_df['day_of_week'].isin(['Saturday', 'Sunday'])\n",
    "    \n",
    "    # 3. Text complexity metrics\n",
    "    # Simple metrics: word count and average word length\n",
    "    enhanced_df['word_count'] = enhanced_df['text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    def avg_word_length(text):\n",
    "        words = str(text).split()\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    enhanced_df['avg_word_length'] = enhanced_df['text'].apply(avg_word_length)\n",
    "    \n",
    "    # 4. Categorize reviews by text length\n",
    "    def categorize_length(length):\n",
    "        if length == 0:\n",
    "            return 'empty'\n",
    "        elif length < 50:\n",
    "            return 'very_short'\n",
    "        elif length < 200:\n",
    "            return 'short'\n",
    "        elif length < 500:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'long'\n",
    "    \n",
    "    enhanced_df['length_category'] = enhanced_df['text_length'].apply(categorize_length)\n",
    "    \n",
    "    # 5. Version-based features\n",
    "    # Extract major version (e.g., '7.2.0' -> '7')\n",
    "    enhanced_df['major_version'] = enhanced_df['version'].apply(\n",
    "        lambda x: x.split('.')[0] if isinstance(x, str) and '.' in x else 'Unknown'\n",
    "    )\n",
    "    \n",
    "    # 6. Add text preprocessing columns\n",
    "    print(\"Adding text preprocessing columns...\")\n",
    "    try:\n",
    "        # Import NLP preprocessor\n",
    "        sys.path.insert(0, project_root)\n",
    "        from src.modules.preprocessing.nlp_preprocessor import NLPPreprocessor\n",
    "        \n",
    "        # Initialize the preprocessor\n",
    "        preprocessor = NLPPreprocessor({\"enable_lemmatization\": True})\n",
    "        if not hasattr(preprocessor, 'is_initialized') or not preprocessor.is_initialized:\n",
    "            preprocessor.initialize()\n",
    "        \n",
    "        # Apply preprocessing to text column\n",
    "        print(\"Cleaning and normalizing review text...\")\n",
    "        enhanced_df['cleaned_text'] = enhanced_df['text'].apply(\n",
    "            lambda x: preprocessor.clean_text(str(x)) if pd.notna(x) else \"\")\n",
    "        \n",
    "        enhanced_df['normalized_text'] = enhanced_df['cleaned_text'].apply(\n",
    "            lambda x: preprocessor.normalize_text(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        print(\"Text preprocessing complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text preprocessing: {e}\")\n",
    "        print(\"Falling back to basic text cleaning...\")\n",
    "        \n",
    "        # Basic fallback preprocessing if the advanced module fails\n",
    "        import re\n",
    "        import nltk\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            from nltk.corpus import stopwords\n",
    "            stopwords_list = set(stopwords.words('english'))\n",
    "        except:\n",
    "            # If NLTK is not available, use a small set of common stopwords\n",
    "            stopwords_list = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', \n",
    "                              'was', 'were', 'to', 'of', 'in', 'for', 'with'}\n",
    "        \n",
    "        # Simple cleaning function\n",
    "        def basic_clean(text):\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            # Remove special characters and punctuation\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Remove numbers\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "            \n",
    "        # Simple stopword removal\n",
    "        def remove_stopwords(text):\n",
    "            return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "        \n",
    "        # Apply basic cleaning\n",
    "        enhanced_df['cleaned_text'] = enhanced_df['text'].apply(\n",
    "            lambda x: basic_clean(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        # Apply stopword removal for normalization\n",
    "        enhanced_df['normalized_text'] = enhanced_df['cleaned_text'].apply(\n",
    "            lambda x: remove_stopwords(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        print(\"Basic text preprocessing complete.\")\n",
    "    \n",
    "    print(f\"Feature engineering complete. Added {len(enhanced_df.columns) - len(df.columns)} new features\")\n",
    "    return enhanced_df\n",
    "\n",
    "# Add features\n",
    "enhanced_df = add_features(cleaned_df)\n",
    "\n",
    "# Display a summary of the new features\n",
    "if enhanced_df is not None and not enhanced_df.empty:\n",
    "    print(\"\\nNew features added:\")\n",
    "    new_features = set(enhanced_df.columns) - set(cleaned_df.columns)\n",
    "    print(', '.join(sorted(new_features)))\n",
    "    \n",
    "    # Display feature statistics\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(enhanced_df['sentiment'].value_counts())\n",
    "    \n",
    "    print(\"\\nLength category distribution:\")\n",
    "    print(enhanced_df['length_category'].value_counts())\n",
    "    \n",
    "    print(\"\\nMajor version distribution (top 5):\")\n",
    "    print(enhanced_df['major_version'].value_counts().head())\n",
    "    \n",
    "    # Display sample of text preprocessing\n",
    "    print(\"\\nText preprocessing sample:\")\n",
    "    sample_df = enhanced_df[['text', 'cleaned_text', 'normalized_text']].head(2)\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nOriginal: {row['text']}\")\n",
    "        print(f\"Cleaned:  {row['cleaned_text']}\")\n",
    "        print(f\"Normalized: {row['normalized_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Processing and Export\n",
    "\n",
    "This section performs the final processing steps and exports the data to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing final processing...\n",
      "Processed data exported to /Users/dipesh/Local-Projects/indigo-reviews-ai/data/processed/processed_reviews.csv\n",
      "\n",
      "Final Dataset Summary:\n",
      "- Shape: (10000, 22)\n",
      "- Memory usage: 1572.39 KB\n",
      "- Column list: review_id, author, rating, text, version, thumbsUpCount, replyContent, repliedAt, date, timestamp, text_length, sentiment, year, month, day_of_week, is_weekend, word_count, avg_word_length, length_category, major_version, cleaned_text, normalized_text\n",
      "\n",
      "Sample of processed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>length_category</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0033822f-bb10-43a1-ad24-a9504001edba</td>\n",
       "      <td>2025-05-07 12:19:43</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>very nice</td>\n",
       "      <td>very_short</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f15ac7dd-411c-4455-bba5-f08ea195fbf6</td>\n",
       "      <td>2025-05-07 09:17:44</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>very bad interface, booked a normal fare ticke...</td>\n",
       "      <td>short</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6c32819-9275-419a-8ed5-268cf123ca7d</td>\n",
       "      <td>2025-05-07 07:59:32</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good update information</td>\n",
       "      <td>very_short</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab99aad1-b18e-47e7-b52f-1436daca2da8</td>\n",
       "      <td>2025-05-07 00:23:10</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sucks big time</td>\n",
       "      <td>very_short</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c5e97ce3-743f-4028-9b01-8079df9fa724</td>\n",
       "      <td>2025-05-06 23:16:04</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst app and worst website, its like some chi...</td>\n",
       "      <td>short</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              review_id                date  rating sentiment  \\\n",
       "0  0033822f-bb10-43a1-ad24-a9504001edba 2025-05-07 12:19:43       5  positive   \n",
       "1  f15ac7dd-411c-4455-bba5-f08ea195fbf6 2025-05-07 09:17:44       1  negative   \n",
       "2  b6c32819-9275-419a-8ed5-268cf123ca7d 2025-05-07 07:59:32       5  positive   \n",
       "3  ab99aad1-b18e-47e7-b52f-1436daca2da8 2025-05-07 00:23:10       1  negative   \n",
       "4  c5e97ce3-743f-4028-9b01-8079df9fa724 2025-05-06 23:16:04       1  negative   \n",
       "\n",
       "                                                text length_category  \\\n",
       "0                                          very nice      very_short   \n",
       "1  very bad interface, booked a normal fare ticke...           short   \n",
       "2                            good update information      very_short   \n",
       "3                                     sucks big time      very_short   \n",
       "4  worst app and worst website, its like some chi...           short   \n",
       "\n",
       "   word_count  \n",
       "0           2  \n",
       "1          34  \n",
       "2           3  \n",
       "3           3  \n",
       "4          13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def finalize_and_export(df):\n",
    "    \"\"\"Perform final processing and export the data\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to export.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Performing final processing...\")\n",
    "    final_df = df.copy()\n",
    "    \n",
    "    # Sort by date (newest first)\n",
    "    final_df = final_df.sort_values('date', ascending=False)\n",
    "    \n",
    "    # Reset index\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    # Export to CSV\n",
    "    final_df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "    print(f\"Processed data exported to {PROCESSED_DATA_PATH}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Finalize and export\n",
    "final_df = finalize_and_export(enhanced_df)\n",
    "\n",
    "# Display final dataset info\n",
    "if final_df is not None and not final_df.empty:\n",
    "    print(\"\\nFinal Dataset Summary:\")\n",
    "    print(f\"- Shape: {final_df.shape}\")\n",
    "    print(f\"- Memory usage: {final_df.memory_usage().sum() / 1024:.2f} KB\")\n",
    "    print(f\"- Column list: {', '.join(final_df.columns.tolist())}\")\n",
    "    \n",
    "    # Display sample rows\n",
    "    print(\"\\nSample of processed data:\")\n",
    "    sample_columns = ['review_id', 'date', 'rating', 'sentiment', 'text', 'length_category', 'word_count']\n",
    "    display(final_df[sample_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has performed the following steps:\n",
    "\n",
    "1. Acquired data from the appropriate source (API, CSV, or mock)\n",
    "2. Cleaned the data by handling duplicates and missing values\n",
    "3. Added features for enhanced analysis\n",
    "4. Exported the processed data to `processed_reviews.csv`\n",
    "\n",
    "The processed data can now be used by other notebooks for analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
