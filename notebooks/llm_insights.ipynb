{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Powered Review Insights Generator\n",
    "\n",
    "This notebook demonstrates how to use Large Language Models (LLMs) to generate structured insights from app reviews. This approach:\n",
    "\n",
    "1. Takes a sample of reviews\n",
    "2. Preprocesses and formats them\n",
    "3. Sends the formatted reviews to an LLM (like GPT-4)\n",
    "4. Extracts structured insights about general feedback, issues, and suggestions\n",
    "\n",
    "The code is designed to be cost-effective and efficient, minimizing API calls while providing valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports\n",
    "\n",
    "First, let's import the necessary modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Applying APP_ID from environment: 'in.goindigo.android'\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "\n",
    "# Data analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from src.modules.llm.openai_llm import OpenAILLM\n",
    "    from src.modules.preprocessing.nlp_preprocessor import NLPPreprocessor\n",
    "    from src.config import config\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the 'notebooks' directory.\")\n",
    "    print(\"Project structure should be: app-reviews-ai/notebooks/llm_insights.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "We'll load sample review data and prepare it for analysis. You can use either mock data or real app reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess reviews data if needed\n",
    "def add_text_preprocessing(df):\n",
    "    \"\"\"Add text preprocessing columns if they don't exist\"\"\"\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Check if preprocessing columns already exist\n",
    "    if 'cleaned_text' in processed_df.columns and 'normalized_text' in processed_df.columns:\n",
    "        print(\"Text preprocessing columns already exist\")\n",
    "        return processed_df\n",
    "    \n",
    "    print(\"Adding text preprocessing columns...\")\n",
    "    # This function only adds text preprocessing - it assumes other features exist\n",
    "    try:\n",
    "        # Import NLP preprocessor\n",
    "        sys.path.insert(0, project_root)\n",
    "        from src.modules.preprocessing.nlp_preprocessor import NLPPreprocessor\n",
    "        \n",
    "        # Initialize the preprocessor\n",
    "        preprocessor = NLPPreprocessor({\"enable_lemmatization\": True})\n",
    "        if not hasattr(preprocessor, 'is_initialized') or not preprocessor.is_initialized:\n",
    "            preprocessor.initialize()\n",
    "        \n",
    "        # Apply preprocessing to text column\n",
    "        print(\"Cleaning and normalizing review text...\")\n",
    "        processed_df['cleaned_text'] = processed_df['text'].apply(\n",
    "            lambda x: preprocessor.clean_text(str(x)) if pd.notna(x) else \"\")\n",
    "        \n",
    "        processed_df['normalized_text'] = processed_df['cleaned_text'].apply(\n",
    "            lambda x: preprocessor.normalize_text(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        print(\"Text preprocessing complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text preprocessing: {e}\")\n",
    "        print(\"Falling back to basic text cleaning...\")\n",
    "        \n",
    "        # Basic fallback preprocessing if the advanced module fails\n",
    "        import re\n",
    "        from nltk.corpus import stopwords\n",
    "        \n",
    "        try:\n",
    "            # Try to download NLTK resources if not available\n",
    "            import nltk\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            stopwords_list = set(stopwords.words('english'))\n",
    "        except:\n",
    "            # If NLTK is not available, use a small set of common stopwords\n",
    "            stopwords_list = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', \n",
    "                              'was', 'were', 'to', 'of', 'in', 'for', 'with'}\n",
    "        \n",
    "        # Simple cleaning function\n",
    "        def basic_clean(text):\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            # Remove special characters and punctuation\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Remove numbers\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "            \n",
    "        # Simple stopword removal\n",
    "        def remove_stopwords(text):\n",
    "            return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "        \n",
    "        # Apply basic cleaning\n",
    "        processed_df['cleaned_text'] = processed_df['text'].apply(\n",
    "            lambda x: basic_clean(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        # Apply stopword removal for normalization\n",
    "        processed_df['normalized_text'] = processed_df['cleaned_text'].apply(\n",
    "            lambda x: remove_stopwords(x) if pd.notna(x) else \"\")\n",
    "        \n",
    "        print(\"Basic text preprocessing complete.\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Function to load and prepare review data\n",
    "def load_reviews(use_mock_data=False, use_processed=True, add_preprocessing=False):\n",
    "    \"\"\"Load review data from CSV or generate mock data\"\"\"\n",
    "    # Set up project paths\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    data_dir = os.path.join(project_root, 'data')\n",
    "    raw_csv_path = os.path.join(data_dir, 'reviews.csv')\n",
    "    processed_data_dir = os.path.join(data_dir, 'processed')\n",
    "    processed_csv_path = os.path.join(processed_data_dir, 'processed_reviews.csv')\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "    \n",
    "    if not use_mock_data:\n",
    "        # Try to load preprocessed data first if requested\n",
    "        if use_processed and os.path.exists(processed_csv_path):\n",
    "            try:\n",
    "                print(f\"Loading preprocessed reviews from: {processed_csv_path}\")\n",
    "                df = pd.read_csv(processed_csv_path)\n",
    "                \n",
    "                # Convert date to datetime if it exists\n",
    "                if 'date' in df.columns:\n",
    "                    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "                \n",
    "                print(f\"Successfully loaded {len(df)} preprocessed reviews\")\n",
    "                \n",
    "                # Check if text preprocessing columns exist\n",
    "                has_preprocessing = 'cleaned_text' in df.columns and 'normalized_text' in df.columns\n",
    "                \n",
    "                # If user explicitly requested preprocessing or add_preprocessing is True\n",
    "                if add_preprocessing and not has_preprocessing:\n",
    "                    print(\"Adding text preprocessing columns as requested...\")\n",
    "                    df = add_text_preprocessing(df)\n",
    "                elif not has_preprocessing:\n",
    "                    print(\"Note: Text preprocessing columns (cleaned_text, normalized_text) are missing.\")\n",
    "                    print(\"      This is fine for basic analysis, but for advanced clustering you can\")\n",
    "                    print(\"      set add_preprocessing=True when calling load_reviews().\")\n",
    "                \n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading preprocessed data: {e}\")\n",
    "                print(\"Trying raw data instead...\")\n",
    "        \n",
    "        # Try to load raw data if preprocessed data not available or not requested\n",
    "        if os.path.exists(raw_csv_path):\n",
    "            try:\n",
    "                print(f\"Loading raw reviews from: {raw_csv_path}\")\n",
    "                df = pd.read_csv(raw_csv_path)\n",
    "                \n",
    "                # Convert date to datetime if it exists\n",
    "                if 'date' in df.columns:\n",
    "                    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "                \n",
    "                print(f\"Successfully loaded {len(df)} raw reviews\")\n",
    "                \n",
    "                # Since this is raw data, we should add basic preprocessing even if not requested\n",
    "                print(\"Processing raw data...\")\n",
    "                processed_df = add_text_preprocessing(df)\n",
    "                \n",
    "                # Save processed data for future use\n",
    "                try:\n",
    "                    processed_df.to_csv(processed_csv_path, index=False)\n",
    "                    print(f\"Saved preprocessed data to: {processed_csv_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not save preprocessed data: {e}\")\n",
    "                \n",
    "                return processed_df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading raw data: {e}\")\n",
    "                print(\"Falling back to mock data...\")\n",
    "    \n",
    "    # Generate mock data if requested or if real data loading failed\n",
    "    print(\"Generating mock review data...\")\n",
    "    \n",
    "    # Import the appropriate module to generate mock reviews\n",
    "    try:\n",
    "        from src.modules.acquisition.google_play import GooglePlayAcquisition\n",
    "        google_play = GooglePlayAcquisition({})\n",
    "        mock_df = google_play.get_mock_reviews(500)\n",
    "        print(f\"Generated {len(mock_df)} mock reviews\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating mock data: {e}\")\n",
    "        # Create a very simple mock dataset\n",
    "        mock_data = []\n",
    "        for i in range(500):\n",
    "            sentiment = random.choice(['positive', 'negative', 'neutral'])\n",
    "            rating = 5 if sentiment == 'positive' else (1 if sentiment == 'negative' else 3)\n",
    "            if sentiment == 'positive':\n",
    "                text = random.choice([\n",
    "                    \"Great app, love it!\",\n",
    "                    \"This app is fantastic. Everything works perfectly.\",\n",
    "                    \"Best airline app I've used. Very easy to book flights.\"\n",
    "                ])\n",
    "            elif sentiment == 'negative':\n",
    "                text = random.choice([\n",
    "                    \"Terrible app, constantly crashes.\",\n",
    "                    \"Cannot book flights. Always get an error at payment.\",\n",
    "                    \"Worst app ever. Customer service is non-existent.\"\n",
    "                ])\n",
    "            else:\n",
    "                text = random.choice([\n",
    "                    \"App is okay. Could use some improvements.\",\n",
    "                    \"Works for basic tasks but lacks some features.\",\n",
    "                    \"Average app experience. Nothing special.\"\n",
    "                ])\n",
    "            \n",
    "            mock_data.append({\n",
    "                'review_id': f\"mock_{i}\",\n",
    "                'text': text,\n",
    "                'rating': rating,\n",
    "                'date': pd.to_datetime('now') - pd.Timedelta(days=random.randint(1, 365))\n",
    "            })\n",
    "        \n",
    "        mock_df = pd.DataFrame(mock_data)\n",
    "        print(f\"Generated {len(mock_df)} simple mock reviews\")\n",
    "    \n",
    "    # Process the mock data\n",
    "    processed_df = add_text_preprocessing(mock_df)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed reviews from: /Users/dipesh/Local-Projects/indigo-reviews-ai/data/processed/processed_reviews.csv\n",
      "Successfully loaded 10000 preprocessed reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very nice</td>\n",
       "      <td>very nice</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>very bad interface, booked a normal fare ticke...</td>\n",
       "      <td>very bad interface booked a normal fare ticket...</td>\n",
       "      <td>bad interface booked normal fare ticket even s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good update information</td>\n",
       "      <td>good update information</td>\n",
       "      <td>good update information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sucks big time</td>\n",
       "      <td>sucks big time</td>\n",
       "      <td>suck big time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst app and worst website, its like some chi...</td>\n",
       "      <td>worst app and worst website its like some chil...</td>\n",
       "      <td>worst app worst website like child developed w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                          very nice   \n",
       "1  very bad interface, booked a normal fare ticke...   \n",
       "2                            good update information   \n",
       "3                                     sucks big time   \n",
       "4  worst app and worst website, its like some chi...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0                                          very nice   \n",
       "1  very bad interface booked a normal fare ticket...   \n",
       "2                            good update information   \n",
       "3                                     sucks big time   \n",
       "4  worst app and worst website its like some chil...   \n",
       "\n",
       "                                     normalized_text  \n",
       "0                                               nice  \n",
       "1  bad interface booked normal fare ticket even s...  \n",
       "2                            good update information  \n",
       "3                                      suck big time  \n",
       "4  worst app worst website like child developed w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset summary:\n",
      "Total reviews: 10000\n",
      "Rating distribution:\n",
      "rating\n",
      "1    3688\n",
      "2     549\n",
      "3     539\n",
      "4     822\n",
      "5    4402\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rating percentages:\n",
      "rating\n",
      "1    36.9\n",
      "2     5.5\n",
      "3     5.4\n",
      "4     8.2\n",
      "5    44.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Date range: 2023-08-27 12:35:07 to 2025-05-07 12:19:43\n",
      "\n",
      "Top 15 most frequent words:\n",
      "[('app', 2948), ('good', 2330), ('flight', 1194), ('indigo', 1172), ('booking', 915), ('service', 871), ('time', 771), ('experience', 663), ('worst', 636), ('nice', 557), ('ticket', 537), ('even', 488), ('customer', 444), ('airline', 443), ('excellent', 440)]\n",
      "\n",
      "Using text preprocessing columns for analysis: True\n"
     ]
    }
   ],
   "source": [
    "# Load or generate review data \n",
    "# Set use_mock_data=False to load real data from CSV\n",
    "use_mock_data = False\n",
    "use_processed = True     # Set to True to use the pre-processed data from data_preprocessing.ipynb\n",
    "add_preprocessing = True  # Set to True to add text preprocessing columns if missing\n",
    "\n",
    "# Load the data\n",
    "reviews_df = load_reviews(\n",
    "    use_mock_data=use_mock_data, \n",
    "    use_processed=use_processed,\n",
    "    add_preprocessing=add_preprocessing\n",
    ")\n",
    "\n",
    "# Check if we have text preprocessing columns\n",
    "has_preprocessing = 'cleaned_text' in reviews_df.columns and 'normalized_text' in reviews_df.columns\n",
    "\n",
    "# Display the first few reviews\n",
    "if reviews_df is not None:\n",
    "    # Display with preprocessing columns if available\n",
    "    if has_preprocessing:\n",
    "        display(reviews_df[['text', 'cleaned_text', 'normalized_text']].head())\n",
    "    else:\n",
    "        display(reviews_df[['text', 'rating', 'date']].head())\n",
    "    \n",
    "    # Display dataset summary\n",
    "    print(\"\\nDataset summary:\")\n",
    "    print(f\"Total reviews: {len(reviews_df)}\")\n",
    "    rating_counts = reviews_df['rating'].value_counts().sort_index()\n",
    "    print(f\"Rating distribution:\\n{rating_counts}\")\n",
    "    \n",
    "    # Calculate percentages\n",
    "    rating_percentages = (rating_counts / len(reviews_df) * 100).round(1)\n",
    "    print(f\"\\nRating percentages:\\n{rating_percentages}\")\n",
    "    \n",
    "    # Show date range\n",
    "    print(f\"\\nDate range: {reviews_df['date'].min()} to {reviews_df['date'].max()}\")\n",
    "    \n",
    "    # Show top 10 most frequent words if we have normalized text\n",
    "    if has_preprocessing:\n",
    "        from collections import Counter\n",
    "        all_words = ' '.join(reviews_df['normalized_text'].fillna('').astype(str)).split()\n",
    "        top_words = Counter(all_words).most_common(15)\n",
    "        print(f\"\\nTop 15 most frequent words:\\n{top_words}\")\n",
    "    \n",
    "    # Whether we're using preprocessing in our analysis\n",
    "    print(f\"\\nUsing text preprocessing columns for analysis: {has_preprocessing}\")\n",
    "else:\n",
    "    print(\"Failed to load review data. Please check the paths and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample for Analysis\n",
    "\n",
    "Rather than analyzing all reviews, we'll create a representative sample that captures the distribution of ratings and key topics. This approach:\n",
    "\n",
    "1. Is more cost-effective (fewer tokens sent to the LLM)\n",
    "2. Focuses the analysis on the most relevant reviews\n",
    "3. Ensures we have representation across all rating levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_insight_prompt(\n",
    "    sample_size=150,  # Increased default for larger dataset\n",
    "    rating_focus='balanced',\n",
    "    clustering='auto',\n",
    "    model='gpt-4o',\n",
    "    focus_areas=None,\n",
    "    use_text_preprocessing=False,  # Set to False by default since processed columns might not exist\n",
    "):\n",
    "    \"\"\"Prepare a sample of reviews and format for LLM analysis\"\"\"\n",
    "    \n",
    "    # Step 1: Determine rating weights based on focus\n",
    "    if rating_focus == 'negative':\n",
    "        # Focus on negative reviews (1-2 stars)\n",
    "        rating_weights = {1: 0.4, 2: 0.3, 3: 0.15, 4: 0.1, 5: 0.05}\n",
    "    elif rating_focus == 'positive':\n",
    "        # Focus on positive reviews (4-5 stars)\n",
    "        rating_weights = {1: 0.05, 2: 0.1, 3: 0.15, 4: 0.3, 5: 0.4}\n",
    "    elif rating_focus == 'equal':\n",
    "        # Equal representation across all ratings\n",
    "        rating_weights = {1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2}\n",
    "    else:  # 'balanced' (default)\n",
    "        # Balanced approach that slightly emphasizes extremes\n",
    "        rating_weights = {1: 0.25, 2: 0.15, 3: 0.2, 4: 0.15, 5: 0.25}\n",
    "    \n",
    "    # Step 2: Determine how many reviews to sample for each rating\n",
    "    rating_counts = {}\n",
    "    for rating in range(1, 6):\n",
    "        count = int(sample_size * rating_weights[rating])\n",
    "        rating_counts[rating] = max(1, count)  # Ensure at least 1 review per rating\n",
    "    \n",
    "    # Adjust to match desired sample size\n",
    "    total = sum(rating_counts.values())\n",
    "    if total != sample_size:\n",
    "        # Distribute the difference proportionally\n",
    "        diff = sample_size - total\n",
    "        sorted_ratings = sorted(rating_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i in range(abs(diff)):\n",
    "            rating = sorted_ratings[i % len(sorted_ratings)][0]\n",
    "            rating_counts[rating] += 1 if diff > 0 else -1\n",
    "            rating_counts[rating] = max(1, rating_counts[rating])  # Ensure at least 1\n",
    "    \n",
    "    # Step 3: Sample reviews based on the determined counts\n",
    "    sample_df = pd.DataFrame()\n",
    "    for rating, count in rating_counts.items():\n",
    "        rating_reviews = reviews_df[reviews_df['rating'] == rating]\n",
    "        if len(rating_reviews) > 0:\n",
    "            # If we have fewer reviews than requested, take all of them\n",
    "            if len(rating_reviews) <= count:\n",
    "                sample_df = pd.concat([sample_df, rating_reviews])\n",
    "            else:\n",
    "                # If clustering is enabled, use cluster-based sampling\n",
    "                if clustering != 'none' and len(rating_reviews) > 10:\n",
    "                    # Check if we should use text preprocessing columns\n",
    "                    if use_text_preprocessing and 'normalized_text' in rating_reviews.columns:\n",
    "                        # Use normalized text for better clustering\n",
    "                        text_column = 'normalized_text'\n",
    "                    else:\n",
    "                        # Fall back to raw text\n",
    "                        text_column = 'text'\n",
    "                        \n",
    "                    # Use TF-IDF and K-means to cluster reviews by content\n",
    "                    try:\n",
    "                        # Convert text to TF-IDF features\n",
    "                        vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "                        tfidf_matrix = vectorizer.fit_transform(rating_reviews[text_column].fillna(''))\n",
    "                        \n",
    "                        # Determine number of clusters based on data size\n",
    "                        n_clusters = min(10, max(3, count))\n",
    "                        \n",
    "                        # Apply K-means clustering\n",
    "                        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                        rating_reviews = rating_reviews.copy()  # Create an explicit copy\n",
    "                        rating_reviews.loc[:, 'cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "                        \n",
    "                        # Sample evenly from each cluster\n",
    "                        cluster_samples = []\n",
    "                        for cluster_id in range(n_clusters):\n",
    "                            cluster_reviews = rating_reviews[rating_reviews['cluster'] == cluster_id]\n",
    "                            if len(cluster_reviews) > 0:\n",
    "                                cluster_count = max(1, count // n_clusters)\n",
    "                                cluster_sample = cluster_reviews.sample(\n",
    "                                    min(len(cluster_reviews), cluster_count),\n",
    "                                    random_state=42\n",
    "                                )\n",
    "                                cluster_samples.append(cluster_sample)\n",
    "                        \n",
    "                        # Combine cluster samples\n",
    "                        cluster_sample_df = pd.concat(cluster_samples)\n",
    "                        \n",
    "                        # If we need more reviews to reach the desired count, sample randomly\n",
    "                        if len(cluster_sample_df) < count:\n",
    "                            remaining = count - len(cluster_sample_df)\n",
    "                            remaining_reviews = rating_reviews[~rating_reviews.index.isin(cluster_sample_df.index)]\n",
    "                            if len(remaining_reviews) > 0:\n",
    "                                additional_sample = remaining_reviews.sample(\n",
    "                                    min(len(remaining_reviews), remaining),\n",
    "                                    random_state=42\n",
    "                                )\n",
    "                                cluster_sample_df = pd.concat([cluster_sample_df, additional_sample])\n",
    "                        \n",
    "                        # Add to the full sample\n",
    "                        sample_df = pd.concat([sample_df, cluster_sample_df])\n",
    "                        print(f\"Using topic modeling for clustering\")\n",
    "                        print(f\"Found {n_clusters} topics\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in clustering: {e}\")\n",
    "                        # Fallback to random sampling\n",
    "                        sample_df = pd.concat([sample_df, rating_reviews.sample(count, random_state=42)])\n",
    "                else:\n",
    "                    # Simple random sampling\n",
    "                    sample_df = pd.concat([sample_df, rating_reviews.sample(count, random_state=42)])\n",
    "    \n",
    "    # Step 4: Sort the sample by rating and date\n",
    "    sample_df = sample_df.sort_values(['rating', 'date'], ascending=[False, False])\n",
    "    \n",
    "    # Step 5: Format the reviews for the prompt\n",
    "    formatted_reviews = []\n",
    "    for i, (_, review) in enumerate(sample_df.iterrows()):\n",
    "        # Format each review with rating and date\n",
    "        date_str = review['date'] if 'date' in review and pd.notna(review['date']) else 'Unknown date'\n",
    "        text = review['text'] if 'text' in review and pd.notna(review['text']) else 'No text provided'\n",
    "        rating = review['rating'] if 'rating' in review and pd.notna(review['rating']) else 'No rating'\n",
    "        \n",
    "        # Add version information if available\n",
    "        version_info = \"\"\n",
    "        if 'version' in review and pd.notna(review['version']):\n",
    "            version_info = f\" (Version: {review['version']})\"\n",
    "        elif 'reviewCreatedVersion' in review and pd.notna(review['reviewCreatedVersion']):\n",
    "            version_info = f\" (Version: {review['reviewCreatedVersion']})\"\n",
    "        \n",
    "        formatted_review = f\"Review #{i+1}: {rating}/5 stars on {date_str}{version_info}\\n{text}\\n\"\n",
    "        formatted_reviews.append(formatted_review)\n",
    "    \n",
    "    # Step 6: Create the system prompt\n",
    "    system_prompt = f\"\"\"You are an expert app review analyst specializing in airline mobile applications. Your task is to analyze {len(formatted_reviews)} reviews \n",
    "for the Indigo Airlines mobile app and extract valuable insights. Focus on understanding user sentiment, \n",
    "identifying key issues, and recognizing patterns across the reviews. Your analysis will help improve the \n",
    "app's functionality and user experience for airline passengers.\"\"\"\n",
    "    \n",
    "    # Step 7: Create the user prompt with context and instructions\n",
    "    # Modify focus areas based on input or provide defaults\n",
    "    if not focus_areas:\n",
    "        focus_areas = [\n",
    "            \"App Performance & Stability\", \n",
    "            \"Booking Experience\", \n",
    "            \"Check-in Process\",\n",
    "            \"Payment System\", \n",
    "            \"User Interface & Design\",\n",
    "            \"Flight Information & Updates\"\n",
    "        ]\n",
    "    else:\n",
    "        # Convert to list regardless of input type (string, tuple, etc.)\n",
    "        focus_areas = list(focus_areas)\n",
    "\n",
    "    focus_areas_text = \"\\n- \".join([\"\"] + focus_areas)\n",
    "    \n",
    "    custom_prompt = f\"\"\"Please analyze the following collection of {len(formatted_reviews)} reviews for the Indigo Airlines mobile app and provide comprehensive insights.\n",
    "\n",
    "Focus on these key areas:{focus_areas_text}\n",
    "\n",
    "For your analysis, please:\n",
    "1. Summarize the overall sentiment and main themes in the reviews\n",
    "2. Identify the most significant issues users are facing with the airline app\n",
    "3. Extract specific feature requests and constructive feedback\n",
    "4. Note any patterns related to app versions, flight experiences, or booking processes\n",
    "5. Suggest 3-5 specific, actionable improvements based on user feedback\n",
    "6. Highlight any positive aspects that could be amplified or extended\n",
    "\n",
    "Format your response as a well-structured report with clear sections and bullet points where appropriate. Include relevant quotes from reviews to support your findings.\n",
    "\n",
    "Here are the reviews to analyze:\n",
    "\n",
    "{''.join(formatted_reviews)}\n",
    "\n",
    "Based on these reviews, please provide your comprehensive analysis and specific, actionable recommendations for improving the Indigo Airlines mobile app.\"\"\"\n",
    "    \n",
    "    print(f\"Final sample size: {len(formatted_reviews)} reviews\")\n",
    "    \n",
    "    # Return all the prepared data\n",
    "    return {\n",
    "        \"sample_size\": sample_size,\n",
    "        \"rating_focus\": rating_focus,\n",
    "        \"clustering\": clustering,\n",
    "        \"model\": model,\n",
    "        \"focus_areas\": focus_areas,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"custom_prompt\": custom_prompt,\n",
    "        \"sample_df\": sample_df,\n",
    "        \"formatted_reviews\": formatted_reviews\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Estimation Function\n",
    "\n",
    "This function helps estimate the number of tokens and associated cost before making an actual API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_limit(prompt, system_prompt=\"\", model=\"gpt-4o\", response_tokens=1800):\n",
    "    \"\"\"Check token limits and estimate costs\"\"\"\n",
    "    # Model limits\n",
    "    model_limits = {\n",
    "        \"gpt-3.5-turbo\": 16385, \"gpt-3.5-turbo-16k\": 16385,\n",
    "        \"gpt-4\": 8192, \"gpt-4o\": 128000, \"gpt-4-turbo\": 128000  \n",
    "    }\n",
    "    # Cost per 1K tokens\n",
    "    model_costs = {\n",
    "        \"gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},\n",
    "        \"gpt-3.5-turbo-16k\": {\"input\": 0.003, \"output\": 0.004},\n",
    "        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "        \"gpt-4o\": {\"input\": 0.01, \"output\": 0.02},\n",
    "        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03}\n",
    "    }\n",
    "    # Get limit and estimate tokens\n",
    "    token_limit = model_limits.get(model, 8192)\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    system_tokens = len(system_prompt) // 4\n",
    "    total_tokens = prompt_tokens + system_tokens + response_tokens\n",
    "    \n",
    "    # Calculate cost\n",
    "    if model in model_costs:\n",
    "        input_cost = (prompt_tokens + system_tokens) / 1000 * model_costs[model][\"input\"]\n",
    "        output_cost = response_tokens / 1000 * model_costs[model][\"output\"]\n",
    "        total_cost = input_cost + output_cost\n",
    "        cost_str = f\"${total_cost:.4f}\"\n",
    "    else:\n",
    "        cost_str = \"Unknown\"\n",
    "    \n",
    "    # Determine status\n",
    "    if total_tokens > token_limit:\n",
    "        status = \"Exceeds limit\"\n",
    "        percentage = f\"{total_tokens / token_limit * 100:.1f}% (EXCEEDS LIMIT)\"\n",
    "        warning = f\"The total estimated tokens ({total_tokens}) exceeds the model's limit ({token_limit})\"\n",
    "    else:\n",
    "        status = \"Within limit\"\n",
    "        percentage = f\"{total_tokens / token_limit * 100:.1f}%\"\n",
    "        warning = None\n",
    "    \n",
    "    # Return results\n",
    "    result = {\n",
    "        \"prompt_tokens\": prompt_tokens, \"system_tokens\": system_tokens, \n",
    "        \"response_tokens\": response_tokens, \"total_tokens\": total_tokens,\n",
    "        \"model_limit\": token_limit, \"status\": status,\n",
    "        \"percentage_of_limit\": percentage, \"estimated_cost\": cost_str\n",
    "    }\n",
    "    if warning:\n",
    "        result[\"warning\"] = warning\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens_and_cost(prepared_data, model, max_tokens=1800):\n",
    "    \"\"\"Estimate tokens and cost for the prepared prompt\"\"\"\n",
    "    prompt = prepared_data[\"custom_prompt\"]\n",
    "    system_prompt = prepared_data[\"system_prompt\"]\n",
    "    \n",
    "    # Get token estimates\n",
    "    token_check = check_token_limit(\n",
    "        prompt=prompt, \n",
    "        system_prompt=system_prompt,\n",
    "        model=model,\n",
    "        response_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    # Create a nice HTML display\n",
    "    html_result = f\"\"\"\n",
    "    <div style=\"border: 1px solid #ddd; padding: 15px; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "        <h3 style=\"margin-top: 0;\">Token and Cost Estimate</h3>\n",
    "        <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">Model:</td>\n",
    "                <td style=\"padding: 5px;\">{model}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">Prompt Tokens:</td>\n",
    "                <td style=\"padding: 5px;\">{token_check['prompt_tokens']:,}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">System Tokens:</td>\n",
    "                <td style=\"padding: 5px;\">{token_check['system_tokens']:,}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">Response Tokens:</td>\n",
    "                <td style=\"padding: 5px;\">{max_tokens:,}</td>\n",
    "            </tr>\n",
    "            <tr style=\"border-top: 1px solid #ddd;\">\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">Total Tokens:</td>\n",
    "                <td style=\"padding: 5px;\">{token_check['total_tokens']:,}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">Token Limit:</td>\n",
    "                <td style=\"padding: 5px;\">{token_check['model_limit']:,}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 5px; font-weight: bold;\">Limit Usage:</td>\n",
    "                <td style=\"padding: 5px;\">{token_check['percentage_of_limit']}</td>\n",
    "            </tr>\n",
    "            <tr style=\"border-top: 1px solid #ddd; font-weight: bold;\">\n",
    "                <td style=\"padding: 5px; color: #d9534f;\">Estimated Cost:</td>\n",
    "                <td style=\"padding: 5px; color: #d9534f;\">{token_check['estimated_cost']}</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        <div style=\"margin-top: 10px; font-style: italic; color: #666;\">\n",
    "            Based on {prepared_data['sample_size']} reviews with focus on {prepared_data['formatted_reviews'][0][:50]}...\n",
    "        </div>\n",
    "        {f'<div style=\"margin-top: 10px; color: #d9534f; font-weight: bold;\">{token_check[\"warning\"]}</div>' if \"warning\" in token_check else '<div style=\"margin-top: 10px; color: #5cb85c; font-weight: bold;\">âœ“ Within token limits</div>'}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html_result))\n",
    "    \n",
    "    return token_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Insight Generator\n",
    "\n",
    "This function interfaces with the LLM module to generate insights from the prepared review sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe_llm_call(prompt, system_prompt=\"\", max_tokens=1800, temperature=0.2, model=\"gpt-4o\"):\n",
    "    \"\"\"Make a safe call to the LLM with error handling\"\"\"\n",
    "    try:\n",
    "        # Initialize the LLM module\n",
    "        llm = OpenAILLM({\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        })\n",
    "        \n",
    "        # Initialize the module\n",
    "        llm.initialize()\n",
    "        \n",
    "        # Generate text\n",
    "        response = llm.generate_text(\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        # Handle any errors\n",
    "        error_msg = f\"Error generating insights: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        # Create a mock response for testing\n",
    "        mock_response = f\"\"\"\n",
    "ERROR: {error_msg}\n",
    "\n",
    "Since there was an error connecting to the LLM, here's a mock response for testing purposes:\n",
    "\n",
    "# Review Analysis Summary\n",
    "\n",
    "## Overall Sentiment\n",
    "The reviews show a mix of positive and negative sentiment. Many users appreciate the app's functionality but have encountered technical issues.\n",
    "\n",
    "## Key Issues Identified\n",
    "1. App crashes during booking process\n",
    "2. Payment system occasionally charges users twice\n",
    "3. Check-in feature unreliable in some cases\n",
    "\n",
    "## Suggested Improvements\n",
    "1. Improve app stability, particularly during the booking flow\n",
    "2. Fix the payment system to prevent double-charging\n",
    "3. Enhance the UI for better navigation\n",
    "4. Streamline the check-in process\n",
    "5. Improve customer support response time\n",
    "\"\"\"\n",
    "        return mock_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custom_insights(\n",
    "    sample_size=75,\n",
    "    rating_focus='balanced',\n",
    "    clustering='auto',\n",
    "    model='gpt-4o',\n",
    "    focus_areas=None,\n",
    "    max_tokens=1800\n",
    "):\n",
    "    \"\"\"Generate insights with the given parameters\"\"\"\n",
    "    print(f\"Generating insights with: sample_size={sample_size}, rating_focus={rating_focus}, clustering={clustering}, model={model}\")\n",
    "    \n",
    "    # Prepare the prompt and sample\n",
    "    prepared_data = prepare_insight_prompt(\n",
    "        sample_size=sample_size,\n",
    "        rating_focus=rating_focus,\n",
    "        clustering=clustering,\n",
    "        model=model,\n",
    "        focus_areas=focus_areas\n",
    "    )\n",
    "    \n",
    "    # Estimate tokens and cost\n",
    "    token_check = estimate_tokens_and_cost(prepared_data, model, max_tokens)\n",
    "    \n",
    "    # Check if we're within token limits\n",
    "    if token_check[\"status\"] == \"Exceeds limit\":\n",
    "        print(f\"WARNING: {token_check['warning']}\")\n",
    "        print(\"Consider reducing the sample size or maximum response tokens.\")\n",
    "        while True:\n",
    "            proceed = input(\"Do you want to proceed anyway? (yes/no): \").lower()\n",
    "            if proceed in ['yes', 'y']:\n",
    "                break\n",
    "            elif proceed in ['no', 'n']:\n",
    "                print(\"Operation cancelled.\")\n",
    "                return None\n",
    "            else:\n",
    "                print(\"Please enter 'yes' or 'no'.\")\n",
    "    \n",
    "    # Make the API call\n",
    "    result = make_safe_llm_call(\n",
    "        prompt=prepared_data['custom_prompt'],\n",
    "        system_prompt=prepared_data['system_prompt'],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.2,\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"INSIGHTS ({model.upper()}, {sample_size} REVIEWS, {rating_focus.upper()} FOCUS)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(result)\n",
    "    \n",
    "    return {\n",
    "        \"sample_size\": sample_size,\n",
    "        \"rating_focus\": rating_focus,\n",
    "        \"clustering\": clustering,\n",
    "        \"model\": model,\n",
    "        \"focus_areas\": focus_areas,\n",
    "        \"result\": result,\n",
    "        \"sample_df\": prepared_data['sample_df']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive UI for Generating Insights\n",
    "\n",
    "Let's create an interactive UI that allows customization of parameters without the duplicate output issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsightGenerator:\n",
    "    \"\"\"Class to manage the insight generation UI and process\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the insight generator\"\"\"\n",
    "        # Flag to prevent multiple executions\n",
    "        self._operation_in_progress = False\n",
    "        \n",
    "        # Storage for the last generated insights\n",
    "        self.last_insights = None\n",
    "        \n",
    "        # Create widgets\n",
    "        self._create_widgets()\n",
    "        \n",
    "        # Create the UI layout\n",
    "        self._create_ui()\n",
    "    \n",
    "    def _create_widgets(self):\n",
    "        \"\"\"Create the UI widgets with improved layout\"\"\"\n",
    "        # Sample size widget without description in the widget itself\n",
    "        self.sample_size_widget = widgets.IntSlider(\n",
    "            value=150, min=10, max=1000, step=5,  # Increased default to 150 for larger dataset\n",
    "            description='',  # Empty description since we use a separate label\n",
    "            style={'description_width': '0px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        # Rating focus widget\n",
    "        self.rating_weights_widget = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Focus on 1-2 star reviews', 'negative'),\n",
    "                ('Balanced across all ratings', 'balanced'),\n",
    "                ('Focus on all ratings equally', 'equal'),\n",
    "                ('Focus on 4-5 star reviews', 'positive')\n",
    "            ],\n",
    "            value='balanced',\n",
    "            description='Rating Focus:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        # Clustering widget\n",
    "        self.clustering_widget = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Use existing topics if available', 'auto'),\n",
    "                ('Always create new clusters', 'force_new'),\n",
    "                ('Simple rating-based sampling', 'none')\n",
    "            ],\n",
    "            value='auto',\n",
    "            description='',\n",
    "            style={'description_width': '0px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        # Model widget\n",
    "        self.model_widget = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('GPT-4o (most powerful, more expensive)', 'gpt-4o'),\n",
    "                ('GPT-3.5 Turbo (faster, cheaper)', 'gpt-3.5-turbo'),\n",
    "            ],\n",
    "            value='gpt-4o',\n",
    "            description='',\n",
    "            style={'description_width': '0px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        # Focus areas widget - Updated for airline app\n",
    "        self.focus_area_widget = widgets.SelectMultiple(\n",
    "            options=[\n",
    "                'App Performance & Stability',\n",
    "                'Booking Experience',\n",
    "                'Check-in Process',\n",
    "                'Customer Support',\n",
    "                'Payment System',\n",
    "                'User Interface & Design',\n",
    "                'Flight Information & Updates',\n",
    "                'Loyalty Program & Miles',\n",
    "                'In-app Features',\n",
    "                'Flight Modifications & Cancellations',\n",
    "                'Baggage Information',\n",
    "                'Boarding Pass & Digital Documents'\n",
    "            ],\n",
    "            value=['App Performance & Stability', 'Booking Experience', 'Check-in Process', \n",
    "                   'Payment System', 'User Interface & Design'],\n",
    "            description='Focus Areas:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='500px', height='200px')  # Made taller for more options\n",
    "        )\n",
    "        \n",
    "        # Output areas\n",
    "        self.estimate_output = widgets.Output()\n",
    "        self.insight_output = widgets.Output()\n",
    "        \n",
    "        # Buttons with bigger size\n",
    "        self.estimate_button = widgets.Button(\n",
    "            description='Estimate Tokens & Cost',\n",
    "            button_style='info',\n",
    "            tooltip='Calculate tokens and cost without making an API call',\n",
    "            layout=widgets.Layout(width='250px', height='40px')\n",
    "        )\n",
    "        \n",
    "        self.run_button = widgets.Button(\n",
    "            description='Generate Insights',\n",
    "            button_style='success',\n",
    "            tooltip='Run LLM analysis (will use API credits)',\n",
    "            layout=widgets.Layout(width='250px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Attach handlers\n",
    "        self.estimate_button.on_click(self._on_estimate_button_clicked)\n",
    "        self.run_button.on_click(self._on_run_button_clicked)\n",
    "    \n",
    "    def _create_ui(self):\n",
    "        \"\"\"Create and display the UI with improved layout\"\"\"\n",
    "        # Create configuration section\n",
    "        config_box = widgets.VBox([\n",
    "            widgets.HTML(value='<h3>Sample Configuration:</h3>'),\n",
    "            widgets.HBox([widgets.Label('Sample Size:'), self.sample_size_widget]),\n",
    "            widgets.HBox([widgets.Label('Rating Focus:'), self.rating_weights_widget]),\n",
    "            widgets.HBox([widgets.Label('Clustering:'), self.clustering_widget]),\n",
    "            widgets.HBox([widgets.Label('Model:'), self.model_widget]),\n",
    "            widgets.HTML(value='<h3>Optional Focus Areas:</h3>'),\n",
    "            self.focus_area_widget,\n",
    "        ])\n",
    "        \n",
    "        # Create action buttons section - now in horizontal layout at the bottom\n",
    "        button_box = widgets.HBox([\n",
    "            self.estimate_button,\n",
    "            self.run_button\n",
    "        ], layout=widgets.Layout(\n",
    "            justify_content='center',\n",
    "            margin='20px 0'\n",
    "        ))\n",
    "        \n",
    "        # Create output section\n",
    "        output_box = widgets.VBox([\n",
    "            widgets.HTML(value='<h3>Results:</h3>'),\n",
    "            self.estimate_output,\n",
    "            self.insight_output\n",
    "        ])\n",
    "        \n",
    "        # Organize the complete layout - stacked vertically\n",
    "        main_ui = widgets.VBox([\n",
    "            config_box,\n",
    "            button_box,\n",
    "            output_box\n",
    "        ])\n",
    "        \n",
    "        # Display heading\n",
    "        print(\"CUSTOMIZABLE COST-CONTROLLED INSIGHTS\")\n",
    "        print(\"Configure your parameters, check token usage/cost, and generate insights when ready.\")\n",
    "        print(\"First use 'Estimate Tokens & Cost' to see the impact of your settings without making an API call.\\n\")\n",
    "        \n",
    "        # Display UI\n",
    "        display(main_ui)\n",
    "    \n",
    "    def _on_estimate_button_clicked(self, b):\n",
    "        \"\"\"Handler for estimate button clicks with re-entrancy protection\"\"\"\n",
    "        if self._operation_in_progress:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self._operation_in_progress = True\n",
    "            self.estimate_output.clear_output()\n",
    "            \n",
    "            with self.estimate_output:\n",
    "                # Get widget values\n",
    "                sample_size = self.sample_size_widget.value\n",
    "                rating_focus = self.rating_weights_widget.value\n",
    "                clustering = self.clustering_widget.value\n",
    "                model = self.model_widget.value\n",
    "                focus_areas = self.focus_area_widget.value\n",
    "                \n",
    "                # Display a single message\n",
    "                print(f\"Preparing prompt with: sample_size={sample_size}, rating_focus={rating_focus}, clustering={clustering}, model={model}\")\n",
    "                \n",
    "                # Prepare data\n",
    "                prepared_data = prepare_insight_prompt(\n",
    "                    sample_size=sample_size,\n",
    "                    rating_focus=rating_focus,\n",
    "                    clustering=clustering,\n",
    "                    model=model,\n",
    "                    focus_areas=focus_areas\n",
    "                )\n",
    "                \n",
    "                # Calculate and display token estimates\n",
    "                estimate_tokens_and_cost(prepared_data, model)\n",
    "        except Exception as e:\n",
    "            with self.estimate_output:\n",
    "                print(f\"Error during estimation: {str(e)}\")\n",
    "        finally:\n",
    "            self._operation_in_progress = False\n",
    "    \n",
    "    def _on_run_button_clicked(self, b):\n",
    "        \"\"\"Handler for run button clicks with re-entrancy protection\"\"\"\n",
    "        if self._operation_in_progress:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self._operation_in_progress = True\n",
    "            self.insight_output.clear_output()\n",
    "            \n",
    "            with self.insight_output:\n",
    "                # Get widget values\n",
    "                sample_size = self.sample_size_widget.value\n",
    "                rating_focus = self.rating_weights_widget.value\n",
    "                clustering = self.clustering_widget.value\n",
    "                model = self.model_widget.value\n",
    "                focus_areas = self.focus_area_widget.value\n",
    "                \n",
    "                # Show a single execution message and loading indicator\n",
    "                print(f\"Generating insights with: sample_size={sample_size}, rating_focus={rating_focus}, clustering={clustering}, model={model}\")\n",
    "                print(\"Processing request... this may take 30-60 seconds depending on the model...\")\n",
    "                \n",
    "                # You could also use a more visual loading indicator\n",
    "                for i in range(3):\n",
    "                    print(\"â³\", end=\"\", flush=True)\n",
    "                \n",
    "                # Prepare data\n",
    "                prepared_data = prepare_insight_prompt(\n",
    "                    sample_size=sample_size,\n",
    "                    rating_focus=rating_focus,\n",
    "                    clustering=clustering,\n",
    "                    model=model,\n",
    "                    focus_areas=focus_areas\n",
    "                )\n",
    "                \n",
    "                # Make the API call\n",
    "                result = make_safe_llm_call(\n",
    "                    prompt=prepared_data['custom_prompt'],\n",
    "                    system_prompt=prepared_data['system_prompt'],\n",
    "                    max_tokens=1800,\n",
    "                    temperature=0.2,\n",
    "                    model=model\n",
    "                )\n",
    "\n",
    "                # Create the insights dictionary\n",
    "                self.last_insights = {\n",
    "                    \"sample_size\": sample_size,\n",
    "                    \"rating_focus\": rating_focus,\n",
    "                    \"clustering\": clustering,\n",
    "                    \"model\": model,\n",
    "                    \"focus_areas\": focus_areas,\n",
    "                    \"result\": result,\n",
    "                    \"sample_df\": prepared_data['sample_df']\n",
    "                }\n",
    "                \n",
    "                # Display results manually once\n",
    "                print(\"\\n\" + \"=\" * 50)\n",
    "                print(f\"INSIGHTS ({model.upper()}, {sample_size} REVIEWS, {rating_focus.upper()} FOCUS)\")\n",
    "                print(\"=\" * 50)\n",
    "                print(result)\n",
    "\n",
    "                # Add a message about saving the insights\n",
    "                print(\"\\nTo save these insights to a file, run: save_insights(insight_generator.last_insights)\")\n",
    "        except Exception as e:\n",
    "            with self.insight_output:\n",
    "                print(f\"Error generating insights: {str(e)}\")\n",
    "                print(\"\\nDebug information:\")\n",
    "                print(f\"- Exception type: {type(e).__name__}\")\n",
    "                print(f\"- Error message: {str(e)}\")\n",
    "                print(\"\\nTroubleshooting:\")\n",
    "                print(\"- Check your OpenAI API key and internet connection\")\n",
    "                print(\"- Make sure the OpenAI API is available\")\n",
    "                print(\"- Verify that the model you selected is available in your account\")\n",
    "        finally:\n",
    "            self._operation_in_progress = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOMIZABLE COST-CONTROLLED INSIGHTS\n",
      "Configure your parameters, check token usage/cost, and generate insights when ready.\n",
      "First use 'Estimate Tokens & Cost' to see the impact of your settings without making an API call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc25ba909b2e489e9bedaf68546246ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>Sample Configuration:</h3>'), HBox(children=(Label(value='Sampleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and display the UI\n",
    "insight_generator = InsightGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Insight Generation\n",
    "\n",
    "If you prefer to generate insights without the UI, you can use the `generate_custom_insights` function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate insights manually with custom parameters\n",
    "# Set to False to see the example without actually running it\n",
    "run_example = False\n",
    "\n",
    "if run_example:\n",
    "    insights = generate_custom_insights(\n",
    "        sample_size=150,        # Increased sample size for our larger dataset\n",
    "        rating_focus='balanced',  # Balanced approach, getting reviews across all ratings\n",
    "        clustering='auto',     # Use automatic clustering\n",
    "        model='gpt-4o',        # Use GPT-4o model\n",
    "        focus_areas=[\n",
    "            'App Performance & Stability',\n",
    "            'Booking Experience', \n",
    "            'Check-in Process',\n",
    "            'Payment System',\n",
    "            'User Interface & Design',\n",
    "            'Flight Information & Updates'\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    print(\"\"\"\n",
    "# Manual Insight Generation Example\n",
    "# To run this analysis, change run_example to True and execute the cell\n",
    "\n",
    "insights = generate_custom_insights(\n",
    "    sample_size=150,        # Number of reviews to sample\n",
    "    rating_focus='balanced',  # Get a mix of all ratings\n",
    "    clustering='auto',     # Use automatic clustering\n",
    "    model='gpt-4o',        # Use GPT-4o model\n",
    "    focus_areas=[\n",
    "        'App Performance & Stability',\n",
    "        'Booking Experience', \n",
    "        'Check-in Process',\n",
    "        'Payment System',\n",
    "        'User Interface & Design',\n",
    "        'Flight Information & Updates'\n",
    "    ]\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Insights\n",
    "\n",
    "You can save the generated insights to a file for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights saved to: /Users/dipesh/Local-Projects/indigo-reviews-ai/reports/insights/insights_negative_600_2025-05-08_14-56-52.md\n"
     ]
    }
   ],
   "source": [
    "def save_insights(insights, filename=None):\n",
    "    \"\"\"Save insights to a file\"\"\"\n",
    "    if insights is None:\n",
    "        print(\"No insights to save.\")\n",
    "        return\n",
    "    \n",
    "    # Create filename if not provided\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        focus = insights.get(\"rating_focus\", \"balanced\")\n",
    "        sample_size = insights.get(\"sample_size\", 0)\n",
    "        filename = f\"insights_{focus}_{sample_size}_{timestamp}.md\"\n",
    "    \n",
    "    # Create reports directory if it doesn't exist\n",
    "    reports_dir = os.path.join(project_root, 'reports', 'insights')\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    \n",
    "    # Full path\n",
    "    filepath = os.path.join(reports_dir, filename)\n",
    "    \n",
    "    # Save the insights\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            # Write metadata\n",
    "            f.write(f\"# Review Insights\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            f.write(f\"## Parameters\\n\\n\")\n",
    "            f.write(f\"- **Sample size**: {insights.get('sample_size', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Rating focus**: {insights.get('rating_focus', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Clustering**: {insights.get('clustering', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Model**: {insights.get('model', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Focus areas**: {', '.join(insights.get('focus_areas', ['N/A']))}\\n\\n\")\n",
    "            \n",
    "            # Write the insights\n",
    "            f.write(f\"## Insights\\n\\n\")\n",
    "            f.write(insights.get(\"result\", \"No insights generated.\"))\n",
    "        \n",
    "        print(f\"Insights saved to: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving insights: {str(e)}\")\n",
    "\n",
    "# Uncomment and execute to save insights from manual insight generation\n",
    "#save_insights(insights)\n",
    "# Save insights from UI based insight generation\n",
    "save_insights(insight_generator.last_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a clean, modular approach to generating insights from app reviews using LLMs. The key benefits of this approach include:\n",
    "\n",
    "1. **Cost efficiency** - Sampling and proper prompt design reduces token usage\n",
    "2. **Representative sampling** - Using clustering ensures diverse review representation\n",
    "3. **Parameter customization** - Flexible configuration for different analysis needs\n",
    "4. **Error prevention** - Robust error handling and duplicate execution prevention\n",
    "5. **Integration with project modules** - Properly interfaces with the project's LLM module\n",
    "\n",
    "You can further extend this notebook by:\n",
    "\n",
    "1. Adding more specialized insight types (competitive analysis, user persona extraction, etc.)\n",
    "2. Implementing automatic insight tracking over time to identify trends\n",
    "3. Creating visualizations based on the LLM-generated insights\n",
    "4. Building a dashboard that combines quantitative metrics with qualitative insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
